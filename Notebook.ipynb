{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ojJn0ovdu4ll"
   },
   "source": [
    "## Web Scraping to Collect Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H3zx1S-tu4lp"
   },
   "source": [
    "You are given the following urls of a website that lists reviews of a well known restaurant in Singapore. \n",
    "\n",
    "You are tasked to scrape the urls for the following data from each review:\n",
    " - The review header\n",
    " - The review footer\n",
    " - The review number\n",
    " - Optional: The date of review\n",
    " - Optional: A Boolean Indicator for Mobile Users\n",
    "\n",
    "A visual aid is given to help you identify the relevant data from the website:\n",
    "\n",
    "![screenshot](https://raw.githubusercontent.com/LC4311/TripAdvisorAnalysis/master/assets/Review_Card.png)\n",
    "\n",
    "Use your knowledge of HTML and BeautifulSoup to obtain the data and pass it into a DataFrame.\n",
    "\n",
    "NOTE: There are cards on the webpage that might have tags with the same class as the tags you are looking for. Be sure to restrict your search to only the relevant html elements.\n",
    "\n",
    "Hint: Use the HTML Inspector using the Developer Tools in Chrome to help you.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uBAAf0LizL9n"
   },
   "source": [
    "You are given the following starter code, which returns a list of urls that you would have to scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LqOAc7Bzu4lr"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re, os, string\n",
    "from datetime import datetime\n",
    "\n",
    "base_url = \"https://www.tripadvisor.com.sg/Restaurant_Review-g294265-d11961130-Reviews-Hawker_Chan-Singapore.html\"\n",
    "\n",
    "urls = [\n",
    "    f\"https://www.tripadvisor.com.sg/Restaurant_Review-g294265-d11961130-Reviews-or{i*10}-Hawker_Chan-Singapore.html\"\n",
    "    for i in range(1, 48)\n",
    "]\n",
    "\n",
    "urls = [base_url, *urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DqfAZtm2zbe0"
   },
   "source": [
    "Define a function to go through the list of urls and scraping each url for the list of review cards:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0Udx-5cGzHaG"
   },
   "outputs": [],
   "source": [
    "def get_HTML(url):\n",
    "    response = requests.get(url)\n",
    "    html = response.text\n",
    "    return html\n",
    "\n",
    "\n",
    "def get_HTML_item_from_urls(urls):\n",
    "    \"\"\"\n",
    "    For each url, look for the review card.\n",
    "    For each review card. Look for the following attributes:\n",
    "    - The rating\n",
    "    - The card header\n",
    "    - The card body\n",
    "    - Optional: The date of review\n",
    "    - Optional: A Boolean Indicator for Mobile Users\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    for url in urls:\n",
    "        html_string = get_HTML(url)\n",
    "        soup = BeautifulSoup(html_string, \"lxml\")\n",
    "        for element in soup(attrs={\"class\": \"ui_column is-9\"}):\n",
    "            obj = {}\n",
    "            # Find Review Number\n",
    "            review = (\n",
    "                element.find(attrs={\"class\": \"ui_bubble_rating bubble_50\"})\n",
    "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_40\"})\n",
    "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_30\"})\n",
    "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_20\"})\n",
    "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_10\"})\n",
    "            )\n",
    "            # If the review number can't be found, then move on. This is not the tag you are looking for\n",
    "            if review == None:\n",
    "                continue\n",
    "            # Get Review number using regex\n",
    "            num = int(\n",
    "                re.findall(\n",
    "                    r'(?<=<span class=\"ui_bubble_rating bubble_)(\\d+).*(?=>)',\n",
    "                    str(review),\n",
    "                )[0]\n",
    "            )\n",
    "            # Find Card Header\n",
    "            card_head = element.find(attrs={\"class\": \"noQuotes\"})\n",
    "            # Find Card Body\n",
    "            card_body = element.find(attrs={\"class\": \"partial_entry\"})\n",
    "            # Find Date of Review\n",
    "            review_date_raw_string = element.find(\n",
    "                attrs={\"class\": \"prw_rup prw_reviews_stay_date_hsx\"}\n",
    "            )\n",
    "            review_date_string = re.findall(\n",
    "                r\"(?<=Date of visit: )(.*)\", review_date_raw_string.text\n",
    "            )[0]\n",
    "            review_date = datetime.strptime(review_date_string, \"%B %Y\").date()\n",
    "            # Find Mobile Users\n",
    "            is_mobile_user = element.find(attrs={\"class\": \"viaMobile\"})\n",
    "            obj.update(\n",
    "                header=card_head.text,\n",
    "                body=card_body.text,\n",
    "                review_num=num,\n",
    "                review_date=review_date,\n",
    "                is_mobile_user=bool(is_mobile_user),\n",
    "            )\n",
    "            res.append(obj)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nGlmBDkZu4l7"
   },
   "outputs": [],
   "source": [
    "# my_text = get_HTML_item_from_urls(urls[:5])\n",
    "my_text = get_HTML_item_from_urls(urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "y73bZ8rVu4mG",
    "outputId": "543208db-1b4d-4962-c915-5694c4618b0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 478 entries, 0 to 477\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   header          478 non-null    object\n",
      " 1   body            478 non-null    object\n",
      " 2   review_num      478 non-null    int64 \n",
      " 3   review_date     478 non-null    object\n",
      " 4   is_mobile_user  478 non-null    bool  \n",
      "dtypes: bool(1), int64(1), object(3)\n",
      "memory usage: 15.5+ KB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.DataFrame(my_text)\n",
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OA3gfnAvu4mR"
   },
   "source": [
    "The data of the results should be published in a pandas data frame format, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "colab_type": "code",
    "id": "sz-dHHX6u4mS",
    "outputId": "a4ed1f64-c044-4c69-cf82-b053cc5ebb7a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>body</th>\n",
       "      <th>review_num</th>\n",
       "      <th>review_date</th>\n",
       "      <th>is_mobile_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Superb Comfort food during COVID-19</td>\n",
       "      <td>With stricter travel advisories across the glo...</td>\n",
       "      <td>50</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Glad I tried it, but didn’t finish it</td>\n",
       "      <td>I got part of the neck of the chicken which I ...</td>\n",
       "      <td>30</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good enough for its price, New Hawker Chan out...</td>\n",
       "      <td>Like others, drawn here for the cheapest Miche...</td>\n",
       "      <td>40</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hawker stall is our preferable taste</td>\n",
       "      <td>We didn’t have in the restaurant at 78 smith s...</td>\n",
       "      <td>40</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good value and quite nice, maybe overrated</td>\n",
       "      <td>I'll start by saying this is probably a bit ov...</td>\n",
       "      <td>40</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              header  \\\n",
       "0                Superb Comfort food during COVID-19   \n",
       "1              Glad I tried it, but didn’t finish it   \n",
       "2  Good enough for its price, New Hawker Chan out...   \n",
       "3               Hawker stall is our preferable taste   \n",
       "4         Good value and quite nice, maybe overrated   \n",
       "\n",
       "                                                body  review_num review_date  \\\n",
       "0  With stricter travel advisories across the glo...          50  2020-03-01   \n",
       "1  I got part of the neck of the chicken which I ...          30  2020-03-01   \n",
       "2  Like others, drawn here for the cheapest Miche...          40  2020-01-01   \n",
       "3  We didn’t have in the restaurant at 78 smith s...          40  2020-02-01   \n",
       "4  I'll start by saying this is probably a bit ov...          40  2020-02-01   \n",
       "\n",
       "   is_mobile_user  \n",
       "0            True  \n",
       "1            True  \n",
       "2           False  \n",
       "3            True  \n",
       "4            True  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XqbcQPGpu4md"
   },
   "source": [
    "Lastly, make sure to persist your data in a `csv` file or equivalent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eEWAE6JwwuSd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n"
     ]
    }
   ],
   "source": [
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WQs1Rwe0u4me"
   },
   "outputs": [],
   "source": [
    "data.to_csv(\"./data/HawkerChanReviews.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qbu0mXwu4mo"
   },
   "source": [
    "## NLP with Review Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "srsfruPdu4mq",
    "outputId": "4aee50ef-0226-4d16-c693-6633a7fa5e9c"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
    "\n",
    "# nltk.download(\"all\")\n",
    "\n",
    "data = pd.read_csv(\"./data/HawkerChanReviews.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 225
    },
    "colab_type": "code",
    "id": "rjyd9DjQu4mw",
    "outputId": "1e95ec18-542d-4de8-fc0d-92bcbd6b977b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 478 entries, 0 to 477\n",
      "Data columns (total 5 columns):\n",
      " #   Column          Non-Null Count  Dtype \n",
      "---  ------          --------------  ----- \n",
      " 0   header          478 non-null    object\n",
      " 1   body            478 non-null    object\n",
      " 2   review_num      478 non-null    int64 \n",
      " 3   review_date     478 non-null    object\n",
      " 4   is_mobile_user  478 non-null    bool  \n",
      "dtypes: bool(1), int64(1), object(3)\n",
      "memory usage: 15.5+ KB\n"
     ]
    }
   ],
   "source": [
    "data.head()\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMIeN7Tvu4m3"
   },
   "source": [
    "Are there any missing values at all?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "id": "bQkALRosu4m4",
    "outputId": "d0952f9b-df53-4b3c-9d4f-da79447d3d8e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing Values:\n",
      "header            False\n",
      "body              False\n",
      "review_num        False\n",
      "review_date       False\n",
      "is_mobile_user    False\n",
      "dtype: bool\n"
     ]
    }
   ],
   "source": [
    "print(\"Missing Values:\", data.isnull().any(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YuGDNl-gu4nA"
   },
   "source": [
    "First, let's scale the review number (represent the scale as 1 - 5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fnlkABZKu4nC"
   },
   "outputs": [],
   "source": [
    "data[\"review_num\"] = data[\"review_num\"] // 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KhDEw6lVu4nI"
   },
   "source": [
    "Next let's make sure that there are no unnecessary whitespaces in the text columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gQRXRgS1u4nK"
   },
   "outputs": [],
   "source": [
    "for column in data.select_dtypes(include=[\"object\"]).columns:\n",
    "    data[column] = data[column].apply(lambda text: text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "colab_type": "code",
    "id": "AbbuMwc9u4nO",
    "outputId": "250ba487-c912-4766-f902-3a584461fce9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>body</th>\n",
       "      <th>review_num</th>\n",
       "      <th>review_date</th>\n",
       "      <th>is_mobile_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Superb Comfort food during COVID-19</td>\n",
       "      <td>With stricter travel advisories across the glo...</td>\n",
       "      <td>5</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Glad I tried it, but didn’t finish it</td>\n",
       "      <td>I got part of the neck of the chicken which I ...</td>\n",
       "      <td>3</td>\n",
       "      <td>2020-03-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good enough for its price, New Hawker Chan out...</td>\n",
       "      <td>Like others, drawn here for the cheapest Miche...</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hawker stall is our preferable taste</td>\n",
       "      <td>We didn’t have in the restaurant at 78 smith s...</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Good value and quite nice, maybe overrated</td>\n",
       "      <td>I'll start by saying this is probably a bit ov...</td>\n",
       "      <td>4</td>\n",
       "      <td>2020-02-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>473</th>\n",
       "      <td>Best chicken ever, at a very affordable price</td>\n",
       "      <td>Not sure I would give a Michelin star to this ...</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>474</th>\n",
       "      <td>Fast food with Michelin star</td>\n",
       "      <td>Long queue to get in, but it is worth. Custome...</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>475</th>\n",
       "      <td>\"Hits the spot😋\"</td>\n",
       "      <td>An expansion of the original hawker stall we h...</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>Worth a visit</td>\n",
       "      <td>Definitely worth a visit, the food was good an...</td>\n",
       "      <td>4</td>\n",
       "      <td>2016-12-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>477</th>\n",
       "      <td>Yes, don't worry about the queue get yourself ...</td>\n",
       "      <td>The success of Mr Chan's hawker stall in China...</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>478 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                header  \\\n",
       "0                  Superb Comfort food during COVID-19   \n",
       "1                Glad I tried it, but didn’t finish it   \n",
       "2    Good enough for its price, New Hawker Chan out...   \n",
       "3                 Hawker stall is our preferable taste   \n",
       "4           Good value and quite nice, maybe overrated   \n",
       "..                                                 ...   \n",
       "473      Best chicken ever, at a very affordable price   \n",
       "474                       Fast food with Michelin star   \n",
       "475                                   \"Hits the spot😋\"   \n",
       "476                                      Worth a visit   \n",
       "477  Yes, don't worry about the queue get yourself ...   \n",
       "\n",
       "                                                  body  review_num  \\\n",
       "0    With stricter travel advisories across the glo...           5   \n",
       "1    I got part of the neck of the chicken which I ...           3   \n",
       "2    Like others, drawn here for the cheapest Miche...           4   \n",
       "3    We didn’t have in the restaurant at 78 smith s...           4   \n",
       "4    I'll start by saying this is probably a bit ov...           4   \n",
       "..                                                 ...         ...   \n",
       "473  Not sure I would give a Michelin star to this ...           5   \n",
       "474  Long queue to get in, but it is worth. Custome...           4   \n",
       "475  An expansion of the original hawker stall we h...           4   \n",
       "476  Definitely worth a visit, the food was good an...           4   \n",
       "477  The success of Mr Chan's hawker stall in China...           5   \n",
       "\n",
       "    review_date  is_mobile_user  \n",
       "0    2020-03-01            True  \n",
       "1    2020-03-01            True  \n",
       "2    2020-01-01           False  \n",
       "3    2020-02-01            True  \n",
       "4    2020-02-01            True  \n",
       "..          ...             ...  \n",
       "473  2017-01-01           False  \n",
       "474  2017-01-01            True  \n",
       "475  2017-01-01            True  \n",
       "476  2016-12-01            True  \n",
       "477  2017-01-01            True  \n",
       "\n",
       "[478 rows x 5 columns]"
      ]
     },
     "execution_count": 320,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l87Gzyfiu4nW"
   },
   "source": [
    "Can we discriminate data based on the header only? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "QhnguHNxu4nZ",
    "outputId": "36d26435-d2cd-4063-f617-3b08b72bc303"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Disappointing                                                4\n",
       "Delicious                                                    2\n",
       "Lunch                                                        2\n",
       "Dinner                                                       2\n",
       "Michelin star?                                               2\n",
       "Very good                                                    2\n",
       "Nothing special                                              2\n",
       "Signature dish cold, measly portion with fragmented bones    1\n",
       "Am I Missing Something???                                    1\n",
       "Super Sedap Soya Chicken Noodle’s                            1\n",
       "Disappointing . . .                                          1\n",
       "Underwhelmed                                                 1\n",
       "Authentic dishes and worth waiting for                       1\n",
       "Very delicious asian noodle                                  1\n",
       "Best chicken rice ever                                       1\n",
       "Name: header, dtype: int64"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_counts_header = data[\"header\"].value_counts()\n",
    "value_counts_header.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 551
    },
    "colab_type": "code",
    "id": "HX_jfHotu4nf",
    "outputId": "dad66ab9-977d-46ab-ef2a-1241763f328f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>header</th>\n",
       "      <th>body</th>\n",
       "      <th>review_num</th>\n",
       "      <th>review_date</th>\n",
       "      <th>is_mobile_user</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>Nothing special</td>\n",
       "      <td>If you like rice, cold chicken and soy sauce t...</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>Nothing special</td>\n",
       "      <td>The Rice and Chicken signature dish is good bu...</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>Michelin star?</td>\n",
       "      <td>My husband and I wanted to try this place as w...</td>\n",
       "      <td>3</td>\n",
       "      <td>2019-07-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Michelin star?</td>\n",
       "      <td>This is a very popular restaurant at the top o...</td>\n",
       "      <td>4</td>\n",
       "      <td>2019-05-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>113</th>\n",
       "      <td>Disappointing</td>\n",
       "      <td>Perhaps I expected too much but having spent t...</td>\n",
       "      <td>2</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>Very good</td>\n",
       "      <td>Belly pork on the pork and noodles dish was su...</td>\n",
       "      <td>5</td>\n",
       "      <td>2019-03-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>Disappointing</td>\n",
       "      <td>Had high expectations but so disappointed. Lon...</td>\n",
       "      <td>1</td>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>171</th>\n",
       "      <td>Delicious</td>\n",
       "      <td>We braved the long line to try this.  It was d...</td>\n",
       "      <td>3</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>239</th>\n",
       "      <td>Lunch</td>\n",
       "      <td>Had lunch with two colleagues. The three of us...</td>\n",
       "      <td>4</td>\n",
       "      <td>2018-06-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>339</th>\n",
       "      <td>Disappointing</td>\n",
       "      <td>Queued for 40 mins to get lukewarm chicken ric...</td>\n",
       "      <td>1</td>\n",
       "      <td>2017-12-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>Dinner</td>\n",
       "      <td>I feel that everyone has their personal standa...</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>Disappointing</td>\n",
       "      <td>Didn't mind the long line outside, that was ex...</td>\n",
       "      <td>2</td>\n",
       "      <td>2017-09-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>425</th>\n",
       "      <td>Delicious</td>\n",
       "      <td>We tried to find the old stall but gave up aft...</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>426</th>\n",
       "      <td>Dinner</td>\n",
       "      <td>The Soy Chicken was nice. The rice was fragran...</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-05-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>444</th>\n",
       "      <td>Lunch</td>\n",
       "      <td>Lunched at Michelin's stars awarded Hawker Cha...</td>\n",
       "      <td>5</td>\n",
       "      <td>2017-04-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>Very good</td>\n",
       "      <td>A very good chicken and rice restaurant. Long ...</td>\n",
       "      <td>4</td>\n",
       "      <td>2017-01-01</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              header                                               body  \\\n",
       "60   Nothing special  If you like rice, cold chicken and soy sauce t...   \n",
       "73   Nothing special  The Rice and Chicken signature dish is good bu...   \n",
       "75    Michelin star?  My husband and I wanted to try this place as w...   \n",
       "99    Michelin star?  This is a very popular restaurant at the top o...   \n",
       "113    Disappointing  Perhaps I expected too much but having spent t...   \n",
       "117        Very good  Belly pork on the pork and noodles dish was su...   \n",
       "160    Disappointing  Had high expectations but so disappointed. Lon...   \n",
       "171        Delicious  We braved the long line to try this.  It was d...   \n",
       "239            Lunch  Had lunch with two colleagues. The three of us...   \n",
       "339    Disappointing  Queued for 40 mins to get lukewarm chicken ric...   \n",
       "383           Dinner  I feel that everyone has their personal standa...   \n",
       "385    Disappointing  Didn't mind the long line outside, that was ex...   \n",
       "425        Delicious  We tried to find the old stall but gave up aft...   \n",
       "426           Dinner  The Soy Chicken was nice. The rice was fragran...   \n",
       "444            Lunch  Lunched at Michelin's stars awarded Hawker Cha...   \n",
       "471        Very good  A very good chicken and rice restaurant. Long ...   \n",
       "\n",
       "     review_num review_date  is_mobile_user  \n",
       "60            2  2019-07-01           False  \n",
       "73            3  2019-07-01            True  \n",
       "75            3  2019-07-01           False  \n",
       "99            4  2019-05-01            True  \n",
       "113           2  2019-03-01            True  \n",
       "117           5  2019-03-01            True  \n",
       "160           1  2018-11-01            True  \n",
       "171           3  2017-12-01            True  \n",
       "239           4  2018-06-01            True  \n",
       "339           1  2017-12-01            True  \n",
       "383           4  2017-09-01            True  \n",
       "385           2  2017-09-01            True  \n",
       "425           4  2017-05-01           False  \n",
       "426           5  2017-05-01            True  \n",
       "444           5  2017-04-01            True  \n",
       "471           4  2017-01-01            True  "
      ]
     },
     "execution_count": 322,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[data.header.isin(value_counts_header[value_counts_header > 1].index)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7qPoZPnVu4nm"
   },
   "source": [
    "Perhaps not, from the instances where the headers were the identical we had differing review numbers. However, the range between the reviews are close - indicating that commenters who made similar comments had the similar general viewpoints in their ratings. This does suggest that more information is needed from the body to supplement the analysis of review patterns. The last thing to note is that frequency of synonymous headers are rare and since the reviews are similar, it might be forgiveable to get away with just analysing the headers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ufacJgF7u4nn"
   },
   "source": [
    "Let's now consider the effect of the comments on the ratings (the `review_num` column): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PpCrKO75u4no"
   },
   "source": [
    "First, we need to hold out a test set in order to be able to gauge the generalisation ability of our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "1hldIqMPu4np",
    "outputId": "3161beaa-81de-435a-d288-a24af36309d2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382, 2) (382,)\n",
      "(96, 2) (96,)\n"
     ]
    }
   ],
   "source": [
    "# Isolate Target From Predictors\n",
    "\n",
    "pred = data.drop([\"review_num\", \"review_date\", \"is_mobile_user\"], axis=1)\n",
    "target = data.review_num\n",
    "\n",
    "# create training and testing vars using a random seed to reproduce experiment results\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    pred, target, test_size=0.2, random_state=5\n",
    ")\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rBqQ6OUou4nu"
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sLjaWSmbu4nv"
   },
   "source": [
    "### Stopword and Punctuation Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 538
    },
    "colab_type": "code",
    "id": "741psplVu4nw",
    "outputId": "f35a6604-b2fb-4109-bf85-a8e8f038f01e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Body:\n",
      "312    pretty good pork belly chicken good michelin s...\n",
      "296    else say ... 5 michelin star meal little hard ...\n",
      "330    judge local asian food seriously understand pl...\n",
      "176    singapore full hawker food part experience go ...\n",
      "305    went toa payoh outlet ordered mixed platter ro...\n",
      "                             ...                        \n",
      "400    visited tai seng outlet lunch still queue thou...\n",
      "118    sign front says michelin star queues inside at...\n",
      "189    ate 2 nights singapore recently went almost cl...\n",
      "206    good taste tried noodle rice vendor read vendo...\n",
      "355    plain good something would expect michelin sta...\n",
      "Name: body, Length: 382, dtype: object\n",
      "\n",
      "\n",
      "Header:\n",
      "312                       great pork belly so-so chicken\n",
      "296                                      5 michelin star\n",
      "330                                 michel-in michel-out\n",
      "176                                                 hype\n",
      "305                                     good value taste\n",
      "                             ...                        \n",
      "400                          lunch visit tai seng branch\n",
      "118    either michelin greay day mr chan indifferent one\n",
      "189                                   awesome cheap eats\n",
      "206                                                great\n",
      "355             cheapest michelin start restaurant world\n",
      "Name: header, Length: 382, dtype: object\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words_punc(data_string):\n",
    "    all_stopwords = stopwords.words(\"english\")  # from nltk\n",
    "    puncs = list(string.punctuation)\n",
    "    puncs += \"’\"\n",
    "\n",
    "    final = []\n",
    "    text = word_tokenize(data_string.lower())\n",
    "    for token in text:\n",
    "        if token not in all_stopwords and token not in puncs:\n",
    "            final.append(token)\n",
    "\n",
    "    return \" \".join(final)\n",
    "\n",
    "\n",
    "print(\"Body:\", X_train[\"body\"].apply(remove_stop_words_punc), \"\\n\", sep=\"\\n\")\n",
    "print(\"Header:\", X_train[\"header\"].apply(remove_stop_words_punc), \"\\n\", sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sbpT6L6Ku4n1"
   },
   "source": [
    "### Stemming  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "QC6benSMu4n1",
    "outputId": "910803fc-e061-423f-dd4d-75001efe4464"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tasti street food , friendli staff'"
      ]
     },
     "execution_count": 325,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_word(data_text):\n",
    "    return \" \".join([porter.stem(word) for word in word_tokenize(data_text)])\n",
    "\n",
    "\n",
    "stem_word(X_train.loc[35, \"header\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZEXuQPPku4n_"
   },
   "source": [
    "### Lemmatizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "colab_type": "code",
    "id": "j5AQjeT7u4oA",
    "outputId": "3cdf7996-e7f5-48ae-ecd0-b4326b1d4c83"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Tasty street food , friendly staff'"
      ]
     },
     "execution_count": 326,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def lemmatize_word(data_text):\n",
    "    return \" \".join([lemma.lemmatize(word) for word in word_tokenize(data_text)])\n",
    "\n",
    "\n",
    "lemmatize_word(X_train.loc[35, \"header\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "a1yIWd0zu4oG",
    "outputId": "8a13d362-3fb8-4902-90a8-4fab930bf58f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "header                            Tasty street food, friendly staff\n",
       "body              I don‘t know why snobbish people are going on ...\n",
       "review_num                                                        5\n",
       "review_date                                              2019-11-01\n",
       "is_mobile_user                                                 True\n",
       "Name: 35, dtype: object"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.loc[35, :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m37t9rTqu4oL"
   },
   "source": [
    "### Putting it all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RJ1OKvrwu4oL"
   },
   "outputs": [],
   "source": [
    "def TextPreprocessorFn(text, strategy=\"lemma\"):\n",
    "    lemma = WordNetLemmatizer()\n",
    "    stemmer = PorterStemmer()\n",
    "    all_stopwords = stopwords.words(\"english\")\n",
    "    puncs = list(string.punctuation)\n",
    "    puncs += [\"“\", \"’\", \"”\", \"-\"]\n",
    "    doc = word_tokenize(text.lower())\n",
    "    removed = [t for t in doc if (t not in puncs) and (t not in all_stopwords)]\n",
    "    if strategy == \"lemma\":\n",
    "        return \" \".join([lemma.lemmatize(token) for token in removed])\n",
    "    else:\n",
    "        return \" \".join([stemmer.stem(token) for token in removed])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "8e-UU6Reu4oP",
    "outputId": "e6f18c64-a755-4c8b-f439-cc326804d2d2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['great pork belly so-so chicken', '5 michelin star',\n",
       "       'michel-in michel-out', 'hype', 'good value taste', 'tragic',\n",
       "       'michelin star', 'original pretty amazing',\n",
       "       'singapore one find chicken rice 2 plate',\n",
       "       'best chicken rice ever', \"wo n't find michellin star street food\",\n",
       "       'fast food michelin star', 'eaten better food', 'much',\n",
       "       'hawker chan famous chicken', 'cool try worth hype',\n",
       "       'good experience', 'michelin star',\n",
       "       'super sedap soya chicken noodle',\n",
       "       \"terrible service horrible food michelin worthy n't bother\",\n",
       "       'family lunch', 'cheap tasty -- life reputation', 'fantastic',\n",
       "       'overhyped', 'rude bad service', 'worth',\n",
       "       'michelin star chicken rice noodle', 'good must visit', 'alright',\n",
       "       'tasty chicken', 'disappointing given hype', 'ok nothing special',\n",
       "       'superb comfort food covid-19', 'nothing special',\n",
       "       'e everything need', 'could use quality control',\n",
       "       'cheapest michelin star restaurant',\n",
       "       \"yes n't worry queue get line\", 'sure fuss', 'awesome meal',\n",
       "       'cheap michelin star restaurant', 'diasppointing overrated',\n",
       "       'hawker stall preferable taste', 'tasteless chicken',\n",
       "       'signature dish cold measly portion fragmented bone',\n",
       "       'cheapest good michelin meal sing',\n",
       "       'feel hardworking chef trying achieve accolade',\n",
       "       'cheapest 1 star michelin -- quite good',\n",
       "       'michelin food 5 singapore dollar/plate', 'worth seeking',\n",
       "       'good noodle', 'worth hype', 'stick chicken', 'cheap n good eats',\n",
       "       'first definitely worth', 'michelin star meal', 'excellent food',\n",
       "       'worth wait', 'michelin well deserved',\n",
       "       'cheapest michelin star experience', 'food fine nothing special',\n",
       "       'great food luke warm', 'totally overrated',\n",
       "       '1-michelin star meal incredible story', 'cold average',\n",
       "       'michelin meh', 'forget hype', 'take care dangerous health',\n",
       "       'excellent food value', 'lovely food price pat',\n",
       "       'unfortunately live hype', 'everything expected', 'disappointing',\n",
       "       'delicious food served', 'tasteless price winning menu',\n",
       "       'yummy chicken rice', 'forget char siu know',\n",
       "       'great food fast aervice', 'super food lived hype',\n",
       "       'take michelin star back', 'dinner hawker chan tai seng branch',\n",
       "       'soya chicken rice ... yum', 'overhyped michelin star craze',\n",
       "       'post mediterranean baltic cruise',\n",
       "       'michelin way good hawker quality', 'short changed',\n",
       "       'michelin star street food really good', 'cold food', 'lunch',\n",
       "       '4-stars go soya sauce chicken', 'michelin starred restaurant',\n",
       "       'quality assured chef chan', 'good ...',\n",
       "       'best soy sauce chicken -- deserving michelin star',\n",
       "       'great food comfy environment', 'disappointed',\n",
       "       'best chicken ever affordable price',\n",
       "       'michelin star food street stall price',\n",
       "       'delicious soy sauce chicken noodle',\n",
       "       'great soya chicken michelin star bargain',\n",
       "       'good cheap michelin experience', 'hawker chan funan',\n",
       "       'hong kong soy sauce chicken',\n",
       "       \"michelin star mcdonald 's better ...\",\n",
       "       'michelin star got excellent food ...', 'delicious',\n",
       "       'choking hazard zero star could', \"tried still n't find special\",\n",
       "       \"rip n't make mistake sucked\", 'disappointing meal',\n",
       "       'didnt wow 2nd n 3rd visit', 'good price', 'must visit singapore',\n",
       "       'place soy sauce chicken', 'chicken melt mouth',\n",
       "       'michelin little joke', 'succulent chicken', 'michelin star',\n",
       "       'bony busy', 'tasty chicken michelin star', 'branch good',\n",
       "       \"'s michelin star\", 'visited funan branch',\n",
       "       \"'s cheap disappointed lot better place eat price\",\n",
       "       'sampler meat platter worth wait line', 'good quality clean fast',\n",
       "       'sorry michelin', 'awesome pork', 'sure 1 michelin star',\n",
       "       'must try mediocre', 'michelin laugh',\n",
       "       'good value quite nice maybe overrated', 'michelin get right',\n",
       "       'authentic chinese fastfood michelin star', 'completely worth',\n",
       "       'seriously', 'little disappointed', 'pleased came experience ..',\n",
       "       'subpar food', 'avoid farce', 'worth waiting', 'good food',\n",
       "       'chan special soya chicken good value', 'bad bad bad bad',\n",
       "       'food speaks volume service', 'hawker chan good',\n",
       "       'okay good expected', 'dinner',\n",
       "       \"still michelin stared restaurant 'm charlie chan earl derr biggers\",\n",
       "       'glad tried finish', 'bit disappointment honest',\n",
       "       'missed michelin mark', 'hawker food', 'incredible value',\n",
       "       'food consistent ...', 'believe hype', 'hype',\n",
       "       'really unique michelin 1 restaurant', 'great food super cheap',\n",
       "       \"order noodle 're hurry\", 'michelin budget',\n",
       "       'good enough price new hawker chan outlet funan mall',\n",
       "       'umm seriously michelin', 'bit strange michelin star',\n",
       "       'best soya sauce chicken charsiew michelin star',\n",
       "       'harrassed gangster china waiter toa payoh branch',\n",
       "       'hype substance', 'michelin preferred',\n",
       "       \"could n't finish it-undercooked chicken\",\n",
       "       'restaurant get awarded michelin star',\n",
       "       'dining cheapest michelin star restaurant',\n",
       "       'believe everything read', 'simple tasty', 'michelin star 👍',\n",
       "       'michelin starred', 'super disappointing',\n",
       "       'toa payoh central outlet', 'cheapest michelin star nice meal',\n",
       "       'excellentl good hawker food', 'michelin star experience',\n",
       "       'cheap meal good quality', 'starred', 'worth visit',\n",
       "       'delicious long long queue',\n",
       "       'cold food poor service fast food style restaurant',\n",
       "       'different kind chicken rice', 'average', 'major let',\n",
       "       'overated hawker chan', 'famous soy sauce chicken',\n",
       "       'hawker michelin food', 'de-licious', 'ridiculous waiting time',\n",
       "       'zero star food worse service', 'bad choise', 'definielty go',\n",
       "       'best', 'overhyped average hawker fare', 'served cold',\n",
       "       'delicious expected', \"'s good 's good\", 'okay', 'good value',\n",
       "       'hype exceptional taste nothing shout', 'great chicken indeed',\n",
       "       'terrible soya chicken', 'cheap eats meat', 'rated',\n",
       "       'outlet kuala lumpur', 'great chicken noodle',\n",
       "       'decent food would recommend', 'little over-hyped good food',\n",
       "       'disappointing', 'cheapest michelin star eatery world',\n",
       "       'decent fast food', 'tad disappointing',\n",
       "       'believe hype go anywhere',\n",
       "       'fresh hawker food sauce given deserved 1 michelin star rating',\n",
       "       'really enjoyable', 'delicious', 'michelin star surely',\n",
       "       'high quality soya chicken', 'michelin star hawker',\n",
       "       'street food good quality', 'experience',\n",
       "       'one star michelin star sure', 'disgusting',\n",
       "       'ordinary michelin star', 'terrible', 'good food overated',\n",
       "       'actually bad ...', 'ok forget michelin star bit', 'worth trying',\n",
       "       'good', 'char siew way better popular soy sauce chicken',\n",
       "       'best hawker stall', 'really', 'best chicken ever- michelin star',\n",
       "       'expectation', 'unfortunatelly ... poor',\n",
       "       'authentic singaporean food', 'much write', 'ok chicken',\n",
       "       'definitely skip toa payoh outlet', 'good chicken rice',\n",
       "       'tasty worth wait', 'good high expectation',\n",
       "       'best cheap meal city',\n",
       "       'manage expectation enjoy good food rather sterile atmosphere',\n",
       "       'chicken char siu', 'late lunch', 'bone red marrow',\n",
       "       'char siew delicious chicken average', 'okay expected',\n",
       "       'outlet toa payoh hub', 'singapore must-do', 'missing something',\n",
       "       'lunch', 'really good char siew soy chicken',\n",
       "       \"cheapest michelin star 'restaurant world\", '', 'disappointing',\n",
       "       'overpriced bad service', 'michelin bib ‘ starred one',\n",
       "       'great food', 'tasty street food friendly staff', 'amazing',\n",
       "       'disappointing michelin experience toa payoh outlet',\n",
       "       'dissapoiting', 'simple yet superb', 'toa payoh outlet',\n",
       "       'cheap affordable michelin lunch', 'really good', 'overstated',\n",
       "       'michelin star', 'good value worth hype',\n",
       "       \"next michelin star force mcdonald 's\",\n",
       "       \"michelin star worthy n't know\", 'liao fan hawker chan',\n",
       "       'unbelievably cheap', 'quick lunch', \"best oily chicken 've ever\",\n",
       "       'delicious chicken rice', 'go experience food', 'could better',\n",
       "       'worm dish terrible hygiene', 'affordable michelin star',\n",
       "       'glorified hawker food', 'michelin star award',\n",
       "       'cheapest michelin-star dish', 'good chicken 1 michelin star',\n",
       "       'cold food new place', 'good', 'cold food ... bad service',\n",
       "       'hawker fare going upmarket', 'good cheap food',\n",
       "       'food standard standardized', 'nice good expected',\n",
       "       'fun novelty great food', 'delicious food .. sooo good',\n",
       "       '45minute wait worth every minute',\n",
       "       \"cheapest michelin star food 'll ever find\",\n",
       "       'tasty springy noodle', 'michelin judge panel thinking',\n",
       "       'pork excellent chicken ok', 'victim success',\n",
       "       'terrible food horrible service', 'experience missed',\n",
       "       'hawker chan smith street',\n",
       "       'temporary bout insanity befalls michelin reviewer', 'sure fuss',\n",
       "       'fun food', 'deserve michelin star 1 hype', 'amazing food',\n",
       "       'love soyasauce chicken noodle', \"'s lived\",\n",
       "       'badly burnt char siew sold', 'inconsistent',\n",
       "       'reputation alone justify accolade', 'try',\n",
       "       '3rd time chef chan shop', 'fresh cheap excellent china town food',\n",
       "       'worth price', 'tasty', 'go early beat queue',\n",
       "       \"best bbq pork pork belly 've\", 'award worthy soy sauce chicken',\n",
       "       'ok expectation', '5/5',\n",
       "       'well hawker chan made history ... wanted share',\n",
       "       'find anything special', 'tasty hawker food', 'over-hyped', 'good',\n",
       "       'tasty chicken best soya sauce noodle', 'average cheap',\n",
       "       'expectation high', \"'s michelin star\", 'michelin star',\n",
       "       'tasty fast', 'cheapest michelin star ever', 'chicken noodle make',\n",
       "       'authentic singapore soya chicken', 'try', 'chicken good',\n",
       "       'alright', 'disappointing', 'reasonable fair', 'great seems',\n",
       "       'incredible food', 'cheap good', 'crowded',\n",
       "       'overrated comfort food',\n",
       "       'get better deal original hawker stall chinatown complex',\n",
       "       'nice food michelin-starred', 'bibendum got one wrong methinks',\n",
       "       \"michelin star restaurant wo n't hurt wallet\", 'great value',\n",
       "       'sunday dinner toa payoh branch', 'worth hype',\n",
       "       'quick cheap reasonable quality meal', 'terrible service', 'must',\n",
       "       'authentic dish worth waiting', 'disappointing',\n",
       "       'good food fine dining', 'good visit', 'live hype',\n",
       "       'michelin star hawker stand',\n",
       "       'cheapest michelins starred restaurant evenr',\n",
       "       'rushed staff equal inconsistent product', 'little actual meat',\n",
       "       'good say', 'over-rated dish crowned michelin', 'prepared queue',\n",
       "       'singapore michelin-starred chicken rice', 'nothing special',\n",
       "       'lunch visit tai seng branch',\n",
       "       'either michelin greay day mr chan indifferent one',\n",
       "       'awesome cheap eats', 'great',\n",
       "       'cheapest michelin start restaurant world'], dtype=object)"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[\"header\"].apply(TextPreprocessorFn, strategy=\"lemma\").values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FB2XJFL4u4oT"
   },
   "source": [
    "### Putting it all together into a Transformer (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vOBwfTPzu4oT"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "\n",
    "\n",
    "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy=\"lemma\"):\n",
    "        \"\"\"\n",
    "        Text preprocessing transformer includes steps:\n",
    "            1. Text normalization\n",
    "            2. Punctuation removal\n",
    "            3. Stop words removal\n",
    "            4. Lemmatization\n",
    "        \"\"\"\n",
    "        self.strategy = strategy\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        self._lemma = WordNetLemmatizer()\n",
    "        self._stemmer = PorterStemmer()\n",
    "        self._all_stopwords = stopwords.words(\"english\")\n",
    "        self._puncs = list(string.punctuation)\n",
    "        self._puncs += [\"“\", \"’\", \"”\", \"-\"]\n",
    "        self._preprocessor = np.vectorize(self._preprocess_text)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, *_):\n",
    "        X_copy = X.copy()\n",
    "        \n",
    "        return X_copy.apply(self._preprocessor).values\n",
    "        \n",
    "\n",
    "    def _preprocess_text(self, text):\n",
    "        doc = word_tokenize(text.lower())\n",
    "        removed_punct = self._remove_punct(doc)\n",
    "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
    "        if self.strategy == \"lemma\":\n",
    "            return self._lemmatize(removed_stop_words)\n",
    "        else:\n",
    "            return self._stem(removed_stop_words)\n",
    "\n",
    "    def _remove_punct(self, doc):\n",
    "        return [t for t in doc if t not in self._puncs]\n",
    "\n",
    "    def _remove_stop_words(self, doc):\n",
    "        return [t for t in doc if t not in self._all_stopwords]\n",
    "\n",
    "    def _lemmatize(self, doc):\n",
    "        return \" \".join([self._lemma.lemmatize(t) for t in doc])\n",
    "\n",
    "    def _stem(self, doc):\n",
    "        return \" \".join([self._stemmer.stem(t) for t in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "EQ3mE61ayq_k",
    "outputId": "0f47592b-f129-4811-ae41-65564e460c34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(382, 2)"
      ]
     },
     "execution_count": 331,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessor = TextPreprocessor()\n",
    "text_preprocessor.fit_transform(X_train).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2nrlv7PKu4oY"
   },
   "source": [
    "### Creating the Training and Testing Tf-Idf Matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7yCWTzU6u4oZ"
   },
   "source": [
    "#### Using the `TfidfVectorizer` itself:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QJeoE6Ru4oa"
   },
   "source": [
    "First, initialize the vectorizer (be sure to add the Text Preprocessing Function as a parameter). Then fit and transform the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_xgdwQXvu4od"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_head = TfidfVectorizer(preprocessor=TextPreprocessorFn )\n",
    "vectorizer_body = TfidfVectorizer(preprocessor=TextPreprocessorFn )\n",
    "tfidf_matrix_header_train = vectorizer_head.fit_transform(X_train[\"header\"])\n",
    "tfidf_matrix_body_train = vectorizer_body.fit_transform(X_train[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "Md5ORxOku4oj",
    "outputId": "8e41e464-5a98-426c-99db-f078793470ea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418\n",
      "1739\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer_head.get_feature_names()))\n",
    "print(len(vectorizer_body.get_feature_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r0INLOVOu4om"
   },
   "source": [
    "Combine the two matrices together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Fn2Ztrx9u4om",
    "outputId": "4bf70926-9d42-4c76-b09b-64f58f4e1af8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382, 2157)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "X_train_vec = scipy.sparse.hstack([tfidf_matrix_header_train, tfidf_matrix_body_train])\n",
    "\n",
    "print(X_train_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kPPvd4QRu4pC"
   },
   "source": [
    "Now using the same vectorizer, transform the test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zFs_-kF2u4pD",
    "outputId": "832c38d0-a367-4d10-faae-17ae2f041fbd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(96, 2157)\n"
     ]
    }
   ],
   "source": [
    "tfidf_matrix_header_test = vectorizer_head.transform(X_test[\"header\"])\n",
    "tfidf_matrix_body_test = vectorizer_body.transform(X_test[\"body\"])\n",
    "\n",
    "X_test_vec = scipy.sparse.hstack([tfidf_matrix_header_test, tfidf_matrix_body_test])\n",
    "\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3Ca2qbV8u4pI"
   },
   "source": [
    "#### Using the Transformer Method (Only applicable if you have created a transformer):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8D56LFdIu4pJ"
   },
   "source": [
    "First, preprocess the data using the transformer. Then use the Tf-Idf vectorizer to fit and transform the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qXRrVO__u4pJ"
   },
   "outputs": [],
   "source": [
    "processor = TextPreprocessor()\n",
    "processed_X_train = X_train.copy()\n",
    "processed_X_train.loc[:, \"header\"] = processor.fit_transform(X_train[\"header\"])\n",
    "processed_X_train.loc[:, \"body\"] = processor.fit_transform(X_train[\"body\"])\n",
    "\n",
    "processed_X_test = X_test.copy()\n",
    "processed_X_test.loc[:, \"header\"] = processor.fit_transform(X_test[\"header\"])\n",
    "processed_X_test.loc[:, \"body\"] = processor.fit_transform(X_test[\"body\"])\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer_head = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "vectorizer_body = TfidfVectorizer(min_df=5, max_df=0.7)\n",
    "tfidf_matrix_header_train = vectorizer_head.fit_transform(processed_X_train[\"header\"])\n",
    "tfidf_matrix_body_train = vectorizer_body.fit_transform(processed_X_train[\"body\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "RCJt_sD0u4pN",
    "outputId": "6577978a-b2a7-4c99-bcbd-d860bf6aac2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57\n",
      "325\n"
     ]
    }
   ],
   "source": [
    "print(len(vectorizer_head.get_feature_names()))\n",
    "print(len(vectorizer_body.get_feature_names()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "246sYQ_Cu4pQ",
    "outputId": "6ccd7f00-b1fe-4869-c8ff-2e866e1654f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(382, 382)\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "X_train_vec = scipy.sparse.hstack([tfidf_matrix_header_train, tfidf_matrix_body_train])\n",
    "\n",
    "print(X_train_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJYahbxvu4pT"
   },
   "outputs": [],
   "source": [
    "tfidf_matrix_header_test = vectorizer_head.transform(processed_X_test[\"header\"])\n",
    "tfidf_matrix_body_test = vectorizer_body.transform(processed_X_test[\"body\"])\n",
    "\n",
    "X_test_vec = scipy.sparse.hstack([tfidf_matrix_header_test, tfidf_matrix_body_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 903
    },
    "colab_type": "code",
    "id": "mpStLzUVu4pW",
    "outputId": "8e5446f9-bd93-4a3a-acf9-ae30ddc85feb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 157)\t0.29373391789748027\n",
      "  (0, 306)\t0.30456928190755983\n",
      "  (0, 137)\t0.4710190245132066\n",
      "  (0, 313)\t0.2958035350168892\n",
      "  (0, 270)\t0.18056820434260834\n",
      "  (0, 174)\t0.16268481506488705\n",
      "  (0, 48)\t0.14985683984885734\n",
      "  (0, 211)\t0.27863544653978\n",
      "  (0, 124)\t0.439352129007043\n",
      "  (0, 214)\t0.3992530299946493\n",
      "  (1, 274)\t0.21069258889269624\n",
      "  (1, 234)\t0.2808655289565301\n",
      "  (1, 230)\t0.15252865358058867\n",
      "  (1, 155)\t0.2679925437198441\n",
      "  (1, 140)\t0.27407071337612515\n",
      "  (1, 156)\t0.28856885551813505\n",
      "  (1, 268)\t0.2679925437198441\n",
      "  (1, 132)\t0.13345289981625905\n",
      "  (1, 196)\t0.2251907310347061\n",
      "  (1, 160)\t0.28856885551813505\n",
      "  (1, 108)\t0.22800539925769311\n",
      "  (1, 131)\t0.2624941704516007\n",
      "  (1, 153)\t0.24460156369856614\n",
      "  (1, 171)\t0.17070544443054525\n",
      "  (1, 239)\t0.21513241402100713\n",
      "  :\t:\n",
      "  (379, 194)\t0.19610887079602962\n",
      "  (379, 255)\t0.18970907543816953\n",
      "  (379, 313)\t0.19886165523662352\n",
      "  (379, 48)\t0.10074517608172555\n",
      "  (380, 271)\t0.4449398916867505\n",
      "  (380, 224)\t0.5056800605012912\n",
      "  (380, 281)\t0.3941780182047317\n",
      "  (380, 296)\t0.4058872656681607\n",
      "  (380, 233)\t0.2549521277509335\n",
      "  (380, 186)\t0.3065527604008468\n",
      "  (380, 124)\t0.25926859504324873\n",
      "  (381, 94)\t0.3407128751758943\n",
      "  (381, 253)\t0.37455736361935454\n",
      "  (381, 187)\t0.312655960349934\n",
      "  (381, 283)\t0.26558596314937594\n",
      "  (381, 181)\t0.2997878730412323\n",
      "  (381, 151)\t0.24404442988860442\n",
      "  (381, 78)\t0.24219545566898223\n",
      "  (381, 186)\t0.20654655096690896\n",
      "  (381, 321)\t0.2707521253061833\n",
      "  (381, 255)\t0.2243986852605251\n",
      "  (381, 207)\t0.3734529296271688\n",
      "  (381, 174)\t0.12936801330812878\n",
      "  (381, 48)\t0.11916712475069476\n",
      "  (381, 124)\t0.17468782212300485\n",
      "  (0, 24)\t1.0\n",
      "  (1, 56)\t0.6299786424490245\n",
      "  (1, 55)\t0.7766124580883853\n",
      "  (2, 48)\t0.7553259574105291\n",
      "  (2, 29)\t0.6553492946985352\n",
      "  (3, 33)\t1.0\n",
      "  (4, 29)\t0.3677061658683197\n",
      "  (4, 28)\t0.6817959850908358\n",
      "  (4, 8)\t0.6324131642339813\n",
      "  (6, 25)\t0.6644727068119055\n",
      "  (6, 5)\t0.7473125329486049\n",
      "  (7, 46)\t0.6437204221429995\n",
      "  (7, 42)\t0.6314106901746939\n",
      "  (7, 9)\t0.43237085753916765\n",
      "  (9, 37)\t0.6775999259234415\n",
      "  (9, 15)\t0.7354307175992492\n",
      "  (12, 7)\t1.0\n",
      "  (13, 40)\t1.0\n",
      "  (14, 31)\t0.6997401963817759\n",
      "  (14, 11)\t0.714397408707222\n",
      "  (15, 23)\t1.0\n",
      "  (17, 22)\t1.0\n",
      "  (20, 46)\t0.7214737478763306\n",
      "  (20, 23)\t0.4946135122659399\n",
      "  (20, 9)\t0.48459581571576943\n",
      "  :\t:\n",
      "  (94, 290)\t0.18687811746970479\n",
      "  (94, 288)\t0.3706599296977301\n",
      "  (94, 243)\t0.22470101847832863\n",
      "  (94, 231)\t0.14073885142488632\n",
      "  (94, 115)\t0.3635718768272302\n",
      "  (94, 105)\t0.12964135288274614\n",
      "  (94, 100)\t0.24104789802679535\n",
      "  (94, 85)\t0.37842401316500374\n",
      "  (94, 66)\t0.4074791890246115\n",
      "  (95, 377)\t0.23285180372911507\n",
      "  (95, 363)\t0.22199234460637046\n",
      "  (95, 361)\t0.24118121786541608\n",
      "  (95, 347)\t0.22368708376238358\n",
      "  (95, 327)\t0.13161129970928584\n",
      "  (95, 309)\t0.3188332464841936\n",
      "  (95, 264)\t0.1711503859951871\n",
      "  (95, 231)\t0.11857657903619215\n",
      "  (95, 228)\t0.2030898706540682\n",
      "  (95, 220)\t0.3538929706556503\n",
      "  (95, 190)\t0.3188332464841936\n",
      "  (95, 189)\t0.15877016841792477\n",
      "  (95, 181)\t0.16011596543028314\n",
      "  (95, 176)\t0.3188332464841936\n",
      "  (95, 124)\t0.29573999654045774\n",
      "  (95, 65)\t0.3433130779013628\n",
      "(96, 382)\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix_body_train)\n",
    "print(X_test_vec)\n",
    "print(X_test_vec.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JCxBgUb2u4pa"
   },
   "source": [
    "### Creating the Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v0r7mmvFu4pa"
   },
   "source": [
    "Let's now create a Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a7tGr-4Uu4pc"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_vec, y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aSQPPz0Lu4pf"
   },
   "source": [
    "Score it against the training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rw2elZpAu4pg",
    "outputId": "b7a075ed-ba76-4e3c-c17e-3a450fa6ff2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8036649214659686"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_train_vec, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "puEf_jFpu4pj"
   },
   "source": [
    "Now use it to predict and score it against the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fJCNvDNPu4pk",
    "outputId": "70756b58-282a-47c8-d681-2867fcb51a8a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40625"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds = model.predict(X_test_vec)\n",
    "model.score(X_test_vec, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xe2O9xrmu4pm"
   },
   "source": [
    "What do the results above tell you about the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oIrU8xJIu4pn"
   },
   "source": [
    "- A Logistic Regression Model (with Regularization) does not really work well for the problem of accurately predicting user ratings.\n",
    "- Overfitting can be seen.\n",
    "- Rationale: linear models might not be geometrically complex enough to separate the classes based on textual information.\n",
    "- Vectorizing int Tf-idf is a simplistic approach, which fails to capture contexts of words. Perhaps more complex transformers such as word embeddings can capture the nuances of the dataset better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LfYlgN06u4po"
   },
   "source": [
    "A proxy can be as such, instead of predicting the actual rating we can say that a prediction does well if it comes within a difference of one star:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "i7bSvTPGu4po",
    "outputId": "fb303180-f89a-4c1c-dfe0-75a207ccdc87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8020833333333334"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.abs(y_test.values - preds) < 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPNnrwV5u4pr"
   },
   "source": [
    "The result above tells us that while the model might be terrible in predicting the actual ratings of users. It does capture the essence of what the user feels based on the comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One benefit of using creating the Transformer (painstakingly) is so that it conforms to the Scikit-Learn estimator standard. This means that you can use it in a pipeline for hyperparameter tuning. Doing so automates the process of choosing hyperparameters (parameters used in the training process but not related to the data) for the optimal set of results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But first, we have to create a meta process for the aggregation of the header and body data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessor = TextPreprocessor()\n",
    "tp = text_preprocessor.fit_transform(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "from sklearn.utils.validation import _deprecate_positional_args\n",
    "\n",
    "class CombineData(TransformerMixin,BaseEstimator):\n",
    "    @_deprecate_positional_args\n",
    "    def __init__(self,**kwargs):\n",
    "        \n",
    "        \"\"\"\n",
    "        Transforms data from multiple columns using the TfIdfVectorizer and combines the data into a sparse matrix. \n",
    "        Check the TfidfVectorizer docstring for more details on initialization parameters.\n",
    "        \"\"\"\n",
    "        self.vectorizer = TfidfVectorizer(**kwargs)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def fit(self,X,y=None,**fit_params):\n",
    "        self._data_arr = []\n",
    "        for col in range(X.shape[1]):\n",
    "            vectorizer = clone(self.vectorizer)\n",
    "            self._data_arr.append(vectorizer.fit(X[:,col],**fit_params))\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, *_):\n",
    "        \n",
    "        return scipy.sparse.hstack([self._data_arr[num].transform(X[:,num]) for num in range(len(self._data_arr))])\n",
    "    \n",
    "    def get_params(self, deep=True):\n",
    "        return self.vectorizer.get_params()\n",
    "\n",
    "    def set_params(self, **parameters):\n",
    "        for parameter, value in parameters.items():\n",
    "            setattr(self.vectorizer, parameter, value)\n",
    "        return self\n",
    "    \n",
    "#     def fit_transform(self, X, y=None, **fit_params):\n",
    "#         \"\"\"\n",
    "#         Fit to data, then transform it.\n",
    "#         Fits transformer to X and y with optional parameters fit_params\n",
    "#         and returns a transformed version of X.\n",
    "#         Parameters\n",
    "#         ----------\n",
    "#         X : {array-like, sparse matrix, dataframe} of shape \\\n",
    "#                 (n_samples, n_features)\n",
    "#         y : ndarray of shape (n_samples,), default=None\n",
    "#             Target values.\n",
    "#         **fit_params : dict\n",
    "#             Additional fit parameters.\n",
    "#         Returns\n",
    "#         -------\n",
    "#         X_new : ndarray array of shape (n_samples, n_features_new)\n",
    "#             Transformed array.\n",
    "#         \"\"\"\n",
    "#         # non-optimized default implementation; override when a better\n",
    "#         # method is possible for a given clustering algorithm\n",
    "#         if y is None:\n",
    "#             # fit method of arity 1 (unsupervised transformation)\n",
    "#             return self.fit(X, **fit_params).transform(X)\n",
    "#         else:\n",
    "#             # fit method of arity 2 (supervised transformation)\n",
    "#             return self.fit(X, y, **fit_params).transform(X)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('process', TextPreprocessor()),\n",
       "                ('tfidf',\n",
       "                 CombineData(analyzer='word', binary=False,\n",
       "                             decode_error='strict',\n",
       "                             dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                             input='content', lowercase=True, max_df=1.0,\n",
       "                             max_features=None, min_df=1, ngram_range=(1, 1),\n",
       "                             norm='l2', preprocessor=None, smooth_idf=True,\n",
       "                             stop_words=None, strip_accents=None,\n",
       "                             sublinear_tf=False,\n",
       "                             token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
       "                             use_idf=True, vocabulary=None)),\n",
       "                ('clf', LogisticRegression())])"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "text_clf_pipeline = Pipeline(\n",
    "    [\n",
    "        (\"process\", TextPreprocessor()),\n",
    "        (\"tfidf\", CombineData()),\n",
    "        (\"clf\", LogisticRegression()),\n",
    "    ]\n",
    ")\n",
    "\n",
    "text_clf_pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.40625"
      ]
     },
     "execution_count": 415,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " text_clf_pipeline.score(X_test,y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<382x382 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 6320 stored elements in COOrdinate format>"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = CombineData(min_df=5, max_df=0.7)\n",
    "\n",
    "c.fit_transform(processed_X_train.values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c._data_arr[0].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "325"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(c._data_arr[1].get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'tfidf__max_df': (0.5, 0.75, 1.0),\n",
    "    'tfidf__max_features': (None, 5000, 10000, 50000),\n",
    "    'tfidf__ngram_range': ((1, 1), (1, 2)),  # unigrams or bigrams\n",
    "    'tfidf__use_idf': (True, False),\n",
    "    'tfidf__norm': ('l1', 'l2'),\n",
    "    'clf__C': (0.000001, 0.00001,0.0001,0.001,0.1,1),\n",
    "    'clf__penalty': ('l1','l2', 'elasticnet'),\n",
    "    'clf__multi_class': ('ovr', 'multinomial'),\n",
    "    'clf__l1_ratio': ('0.25','0.5','0.75')\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing grid search...\n",
      "pipeline: ['process', 'tfidf', 'clf']\n",
      "parameters:\n",
      "{'clf__C': (1e-06, 1e-05, 0.0001, 0.001, 0.1, 1),\n",
      " 'clf__l1_ratio': ('0.25', '0.5', '0.75'),\n",
      " 'clf__multi_class': ('ovr', 'multinomial'),\n",
      " 'clf__penalty': ('l1', 'l2', 'elasticnet'),\n",
      " 'tfidf__max_df': (0.5, 0.75, 1.0),\n",
      " 'tfidf__max_features': (None, 5000, 10000, 50000),\n",
      " 'tfidf__ngram_range': ((1, 1), (1, 2)),\n",
      " 'tfidf__norm': ('l1', 'l2'),\n",
      " 'tfidf__use_idf': (True, False)}\n",
      "done in 0.000\n",
      "\n",
      "Best score: 0.393\n",
      "Best parameters set:\n",
      "\tclf__C: 1\n",
      "\tclf__l1_ratio: '0.25'\n",
      "\tclf__multi_class: 'multinomial'\n",
      "\tclf__penalty: 'l2'\n",
      "\ttfidf__max_df: 0.5\n",
      "\ttfidf__max_features: None\n",
      "\ttfidf__ngram_range: (1, 1)\n",
      "\ttfidf__norm: 'l2'\n",
      "\ttfidf__use_idf: True\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from pprint import pprint\n",
    "from time import time\n",
    "\n",
    "grid_search = GridSearchCV(text_clf_pipeline, parameters, n_jobs=-1, verbose=1)\n",
    "\n",
    "\n",
    "print(\"Performing grid search...\")\n",
    "print(\"pipeline:\", [name for name, _ in text_clf_pipeline.steps])\n",
    "print(\"parameters:\")\n",
    "pprint(parameters)\n",
    "t0 = time()\n",
    "grid_search.fit(X_train, y_train)\n",
    "print(\"done in {:0.3f}\".format((time() - t0)))\n",
    "print()\n",
    "\n",
    "print(\"Best score: {:0.3f}\".format(grid_search.best_score_))\n",
    "print(\"Best parameters set:\")\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"\\t%s: %r\" % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'memory': None, 'steps': [('process', TextPreprocessor()), ('tfidf', CombineData(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "            stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "            token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "            vocabulary=None)), ('clf', LogisticRegression(C=1, l1_ratio='0.25', multi_class='multinomial'))], 'verbose': False, 'process': TextPreprocessor(), 'tfidf': CombineData(analyzer='word', binary=False, decode_error='strict',\n",
      "            dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
      "            lowercase=True, max_df=0.5, max_features=None, min_df=1,\n",
      "            ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "            stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "            token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "            vocabulary=None), 'clf': LogisticRegression(C=1, l1_ratio='0.25', multi_class='multinomial'), 'process__strategy': 'lemma', 'tfidf__analyzer': 'word', 'tfidf__binary': False, 'tfidf__decode_error': 'strict', 'tfidf__dtype': <class 'numpy.float64'>, 'tfidf__encoding': 'utf-8', 'tfidf__input': 'content', 'tfidf__lowercase': True, 'tfidf__max_df': 0.5, 'tfidf__max_features': None, 'tfidf__min_df': 1, 'tfidf__ngram_range': (1, 1), 'tfidf__norm': 'l2', 'tfidf__preprocessor': None, 'tfidf__smooth_idf': True, 'tfidf__stop_words': None, 'tfidf__strip_accents': None, 'tfidf__sublinear_tf': False, 'tfidf__token_pattern': '(?u)\\\\b\\\\w\\\\w+\\\\b', 'tfidf__tokenizer': None, 'tfidf__use_idf': True, 'tfidf__vocabulary': None, 'clf__C': 1, 'clf__class_weight': None, 'clf__dual': False, 'clf__fit_intercept': True, 'clf__intercept_scaling': 1, 'clf__l1_ratio': '0.25', 'clf__max_iter': 100, 'clf__multi_class': 'multinomial', 'clf__n_jobs': None, 'clf__penalty': 'l2', 'clf__random_state': None, 'clf__solver': 'lbfgs', 'clf__tol': 0.0001, 'clf__verbose': 0, 'clf__warm_start': False}\n"
     ]
    }
   ],
   "source": [
    "print(best_parameters)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Module 3 Assessment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
