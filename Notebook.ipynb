{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "Module 3 Assessment.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ojJn0ovdu4ll",
        "colab_type": "text"
      },
      "source": [
        "## Web Scraping to Collect Review Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3zx1S-tu4lp",
        "colab_type": "text"
      },
      "source": [
        "You are given the following urls of a website that lists reviews of a well known restaurant in Singapore. \n",
        "\n",
        "You are tasked to scrape the urls for the following data from each review:\n",
        " - The review header\n",
        " - The review footer\n",
        " - The review number\n",
        " - Optional: The date of review\n",
        " - Optional: A Boolean Indicator for Mobile Users\n",
        "\n",
        "A visual aid is given to help you identify the relevant data from the website:\n",
        "\n",
        "![screenshot](https://raw.githubusercontent.com/LC4311/TripAdvisorAnalysis/master/assets/Review_Card.png)\n",
        "\n",
        "Use your knowledge of HTML and BeautifulSoup to obtain the data and pass it into a DataFrame.\n",
        "\n",
        "NOTE: There are cards on the webpage that might have tags with the same class as the tags you are looking for. Be sure to restrict your search to only the relevant html elements.\n",
        "\n",
        "Hint: Use the HTML Inspector using the Developer Tools in Chrome to help you.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBAAf0LizL9n",
        "colab_type": "text"
      },
      "source": [
        "You are given the following starter code, which returns a list of urls that you would have to scrape:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LqOAc7Bzu4lr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import re, os, string\n",
        "from datetime import datetime\n",
        "\n",
        "base_url = \"https://www.tripadvisor.com.sg/Restaurant_Review-g294265-d11961130-Reviews-Hawker_Chan-Singapore.html\"\n",
        "\n",
        "urls = [\n",
        "    f\"https://www.tripadvisor.com.sg/Restaurant_Review-g294265-d11961130-Reviews-or{i*10}-Hawker_Chan-Singapore.html\"\n",
        "    for i in range(1, 48)\n",
        "]\n",
        "\n",
        "urls = [base_url, *urls]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqfAZtm2zbe0",
        "colab_type": "text"
      },
      "source": [
        "Define a function to go through the list of urls and scraping each url for the list of review cards:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Udx-5cGzHaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def get_HTML(url):\n",
        "    response = requests.get(url)\n",
        "    html = response.text\n",
        "    return html\n",
        "\n",
        "\n",
        "def get_HTML_item_from_urls(urls):\n",
        "    '''\n",
        "    For each url, look for the review card.\n",
        "    For each review card. Look for the following attributes:\n",
        "    - The rating\n",
        "    - The card header\n",
        "    - The card body\n",
        "    - Optional: The date of review\n",
        "    - Optional: A Boolean Indicator for Mobile Users\n",
        "    '''\n",
        "    res = []\n",
        "    for url in urls:\n",
        "        html_string = get_HTML(url)\n",
        "        soup = BeautifulSoup(html_string, \"lxml\")\n",
        "        for element in soup(attrs={\"class\": \"ui_column is-9\"}):\n",
        "            obj = {}\n",
        "            # Find Review Number\n",
        "            review = (\n",
        "                element.find(attrs={\"class\": \"ui_bubble_rating bubble_50\"})\n",
        "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_40\"})\n",
        "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_30\"})\n",
        "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_20\"})\n",
        "                or element.find(attrs={\"class\": \"ui_bubble_rating bubble_10\"})\n",
        "            )\n",
        "            # If the review number can't be found, then move on. This is not the tag you are looking for\n",
        "            if review == None:\n",
        "                continue\n",
        "            #Get Review number using regex\n",
        "            num = int(\n",
        "                re.findall(\n",
        "                    r'(?<=<span class=\"ui_bubble_rating bubble_)(\\d+).*(?=>)',\n",
        "                    str(review),\n",
        "                )[0]\n",
        "            )\n",
        "            #Find Card Header\n",
        "            card_head = element.find(attrs={\"class\": \"noQuotes\"})\n",
        "            #Find Card Body\n",
        "            card_body = element.find(attrs={\"class\": \"partial_entry\"})\n",
        "            #Find Date of Review\n",
        "            review_date_raw_string = element.find(attrs={\"class\": \"prw_rup prw_reviews_stay_date_hsx\"})\n",
        "            review_date_string = re.findall(r'(?<=Date of visit: )(.*)',review_date_raw_string.text)[0]\n",
        "            review_date = datetime.strptime(review_date_string,\"%B %Y\").date()\n",
        "            #Find Mobile Users\n",
        "            is_mobile_user = element.find(attrs={\"class\": \"viaMobile\"})\n",
        "            obj.update(header=card_head.text, body=card_body.text, review_num=num,review_date=review_date,is_mobile_user=bool(is_mobile_user))\n",
        "            res.append(obj)\n",
        "    return res"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGlmBDkZu4l7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# my_text = get_HTML_item_from_urls(urls[:5])\n",
        "my_text = get_HTML_item_from_urls(urls)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y73bZ8rVu4mG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "543208db-1b4d-4962-c915-5694c4618b0a"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.DataFrame(my_text)\n",
        "data.head()\n",
        "data.info()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 478 entries, 0 to 477\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   header          478 non-null    object\n",
            " 1   body            478 non-null    object\n",
            " 2   review_num      478 non-null    int64 \n",
            " 3   review_date     478 non-null    object\n",
            " 4   is_mobile_user  478 non-null    bool  \n",
            "dtypes: bool(1), int64(1), object(3)\n",
            "memory usage: 15.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OA3gfnAvu4mR",
        "colab_type": "text"
      },
      "source": [
        "The data of the results should be published in a pandas data frame format, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sz-dHHX6u4mS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "a4ed1f64-c044-4c69-cf82-b053cc5ebb7a"
      },
      "source": [
        "data.head()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>header</th>\n",
              "      <th>body</th>\n",
              "      <th>review_num</th>\n",
              "      <th>review_date</th>\n",
              "      <th>is_mobile_user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Superb Comfort food during COVID-19</td>\n",
              "      <td>With stricter travel advisories across the glo...</td>\n",
              "      <td>50</td>\n",
              "      <td>2020-03-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Glad I tried it, but didnâ€™t finish it</td>\n",
              "      <td>I got part of the neck of the chicken which I ...</td>\n",
              "      <td>30</td>\n",
              "      <td>2020-03-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Good enough for its price, New Hawker Chan out...</td>\n",
              "      <td>Like others, drawn here for the cheapest Miche...</td>\n",
              "      <td>40</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hawker stall is our preferable taste</td>\n",
              "      <td>We didnâ€™t have in the restaurant at 78 smith s...</td>\n",
              "      <td>40</td>\n",
              "      <td>2020-02-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Good value and quite nice, maybe overrated</td>\n",
              "      <td>I'll start by saying this is probably a bit ov...</td>\n",
              "      <td>40</td>\n",
              "      <td>2020-02-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                              header  ... is_mobile_user\n",
              "0                Superb Comfort food during COVID-19  ...           True\n",
              "1              Glad I tried it, but didnâ€™t finish it  ...           True\n",
              "2  Good enough for its price, New Hawker Chan out...  ...          False\n",
              "3               Hawker stall is our preferable taste  ...           True\n",
              "4         Good value and quite nice, maybe overrated  ...           True\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqbcQPGpu4md",
        "colab_type": "text"
      },
      "source": [
        "Lastly, make sure to persist your data in a `csv` file or equivalent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eEWAE6JwwuSd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!mkdir data"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQs1Rwe0u4me",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data.to_csv(\"./data/HawkerChanReviews.csv\",index=False)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3qbu0mXwu4mo",
        "colab_type": "text"
      },
      "source": [
        "## NLP with Review Data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "srsfruPdu4mq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "4aee50ef-0226-4d16-c693-6633a7fa5e9c"
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.model_selection import train_test_split, cross_val_predict, cross_val_score\n",
        "\n",
        "nltk.download(\"all\")\n",
        "\n",
        "data = pd.read_csv(\"./data/HawkerChanReviews.csv\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/omw.zip.\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjyd9DjQu4mw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "1e95ec18-542d-4de8-fc0d-92bcbd6b977b"
      },
      "source": [
        "data.head()\n",
        "data.info()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 478 entries, 0 to 477\n",
            "Data columns (total 5 columns):\n",
            " #   Column          Non-Null Count  Dtype \n",
            "---  ------          --------------  ----- \n",
            " 0   header          478 non-null    object\n",
            " 1   body            478 non-null    object\n",
            " 2   review_num      478 non-null    int64 \n",
            " 3   review_date     478 non-null    object\n",
            " 4   is_mobile_user  478 non-null    bool  \n",
            "dtypes: bool(1), int64(1), object(3)\n",
            "memory usage: 15.5+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SMIeN7Tvu4m3",
        "colab_type": "text"
      },
      "source": [
        "Are there any missing values at all?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQkALRosu4m4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "d0952f9b-df53-4b3c-9d4f-da79447d3d8e"
      },
      "source": [
        "print(\"Missing Values:\", data.isnull().any(),sep=\"\\n\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Missing Values:\n",
            "header            False\n",
            "body              False\n",
            "review_num        False\n",
            "review_date       False\n",
            "is_mobile_user    False\n",
            "dtype: bool\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YuGDNl-gu4nA",
        "colab_type": "text"
      },
      "source": [
        "First, let's scale the review number (represent the scale as 1 - 5):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnlkABZKu4nC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data[\"review_num\"] = data[\"review_num\"]//10"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhDEw6lVu4nI",
        "colab_type": "text"
      },
      "source": [
        "Next let's make sure that there are no unnecessary whitespaces in the text columns:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQRXRgS1u4nK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for column in data.select_dtypes(include=[\"object\"]).columns:\n",
        "    data[column] = data[column].apply(lambda text: text.strip())"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AbbuMwc9u4nO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "250ba487-c912-4766-f902-3a584461fce9"
      },
      "source": [
        "data"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>header</th>\n",
              "      <th>body</th>\n",
              "      <th>review_num</th>\n",
              "      <th>review_date</th>\n",
              "      <th>is_mobile_user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Superb Comfort food during COVID-19</td>\n",
              "      <td>With stricter travel advisories across the glo...</td>\n",
              "      <td>5</td>\n",
              "      <td>2020-03-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Glad I tried it, but didnâ€™t finish it</td>\n",
              "      <td>I got part of the neck of the chicken which I ...</td>\n",
              "      <td>3</td>\n",
              "      <td>2020-03-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Good enough for its price, New Hawker Chan out...</td>\n",
              "      <td>Like others, drawn here for the cheapest Miche...</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-01-01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Hawker stall is our preferable taste</td>\n",
              "      <td>We didnâ€™t have in the restaurant at 78 smith s...</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-02-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Good value and quite nice, maybe overrated</td>\n",
              "      <td>I'll start by saying this is probably a bit ov...</td>\n",
              "      <td>4</td>\n",
              "      <td>2020-02-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>473</th>\n",
              "      <td>Best chicken ever, at a very affordable price</td>\n",
              "      <td>Not sure I would give a Michelin star to this ...</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>474</th>\n",
              "      <td>Fast food with Michelin star</td>\n",
              "      <td>Long queue to get in, but it is worth. Custome...</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>475</th>\n",
              "      <td>\"Hits the spotðŸ˜‹\"</td>\n",
              "      <td>An expansion of the original hawker stall we h...</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>476</th>\n",
              "      <td>Worth a visit</td>\n",
              "      <td>Definitely worth a visit, the food was good an...</td>\n",
              "      <td>4</td>\n",
              "      <td>2016-12-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>477</th>\n",
              "      <td>Yes, don't worry about the queue get yourself ...</td>\n",
              "      <td>The success of Mr Chan's hawker stall in China...</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>478 rows Ã— 5 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                header  ... is_mobile_user\n",
              "0                  Superb Comfort food during COVID-19  ...           True\n",
              "1                Glad I tried it, but didnâ€™t finish it  ...           True\n",
              "2    Good enough for its price, New Hawker Chan out...  ...          False\n",
              "3                 Hawker stall is our preferable taste  ...           True\n",
              "4           Good value and quite nice, maybe overrated  ...           True\n",
              "..                                                 ...  ...            ...\n",
              "473      Best chicken ever, at a very affordable price  ...          False\n",
              "474                       Fast food with Michelin star  ...           True\n",
              "475                                   \"Hits the spotðŸ˜‹\"  ...           True\n",
              "476                                      Worth a visit  ...           True\n",
              "477  Yes, don't worry about the queue get yourself ...  ...           True\n",
              "\n",
              "[478 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l87Gzyfiu4nW",
        "colab_type": "text"
      },
      "source": [
        "Can we discriminate data based on the header only? "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QhnguHNxu4nZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "36d26435-d2cd-4063-f617-3b08b72bc303"
      },
      "source": [
        "value_counts_header = data[\"header\"].value_counts()\n",
        "value_counts_header.head(15)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Disappointing                             4\n",
              "Delicious                                 2\n",
              "Dinner                                    2\n",
              "Michelin star?                            2\n",
              "Nothing special                           2\n",
              "Lunch                                     2\n",
              "Very good                                 2\n",
              "This is an EXPERIENCE not to be missed    1\n",
              "Good Food Cheap                           1\n",
              "DINNER AT HAWKER CHAN TAI SENG BRANCH     1\n",
              "Go early to beat the queue                1\n",
              "OVERRATED COMFORT FOOD!                   1\n",
              "The famous soy sauce chicken.             1\n",
              "Just bad                                  1\n",
              "Disappointed                              1\n",
              "Name: header, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HX_jfHotu4nf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 551
        },
        "outputId": "dad66ab9-977d-46ab-ef2a-1241763f328f"
      },
      "source": [
        "data.loc[data.header.isin(value_counts_header[value_counts_header > 1].index)]"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>header</th>\n",
              "      <th>body</th>\n",
              "      <th>review_num</th>\n",
              "      <th>review_date</th>\n",
              "      <th>is_mobile_user</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>Nothing special</td>\n",
              "      <td>If you like rice, cold chicken and soy sauce t...</td>\n",
              "      <td>2</td>\n",
              "      <td>2019-07-01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>73</th>\n",
              "      <td>Nothing special</td>\n",
              "      <td>The Rice and Chicken signature dish is good bu...</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-07-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75</th>\n",
              "      <td>Michelin star?</td>\n",
              "      <td>My husband and I wanted to try this place as w...</td>\n",
              "      <td>3</td>\n",
              "      <td>2019-07-01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>99</th>\n",
              "      <td>Michelin star?</td>\n",
              "      <td>This is a very popular restaurant at the top o...</td>\n",
              "      <td>4</td>\n",
              "      <td>2019-05-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>113</th>\n",
              "      <td>Disappointing</td>\n",
              "      <td>Perhaps I expected too much but having spent t...</td>\n",
              "      <td>2</td>\n",
              "      <td>2019-03-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>117</th>\n",
              "      <td>Very good</td>\n",
              "      <td>Belly pork on the pork and noodles dish was su...</td>\n",
              "      <td>5</td>\n",
              "      <td>2019-03-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>160</th>\n",
              "      <td>Disappointing</td>\n",
              "      <td>Had high expectations but so disappointed. Lon...</td>\n",
              "      <td>1</td>\n",
              "      <td>2018-11-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>171</th>\n",
              "      <td>Delicious</td>\n",
              "      <td>We braved the long line to try this.  It was d...</td>\n",
              "      <td>3</td>\n",
              "      <td>2017-12-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>239</th>\n",
              "      <td>Lunch</td>\n",
              "      <td>Had lunch with two colleagues. The three of us...</td>\n",
              "      <td>4</td>\n",
              "      <td>2018-06-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>339</th>\n",
              "      <td>Disappointing</td>\n",
              "      <td>Queued for 40 mins to get lukewarm chicken ric...</td>\n",
              "      <td>1</td>\n",
              "      <td>2017-12-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>383</th>\n",
              "      <td>Dinner</td>\n",
              "      <td>I feel that everyone has their personal standa...</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-09-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>385</th>\n",
              "      <td>Disappointing</td>\n",
              "      <td>Didn't mind the long line outside, that was ex...</td>\n",
              "      <td>2</td>\n",
              "      <td>2017-09-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>425</th>\n",
              "      <td>Delicious</td>\n",
              "      <td>We tried to find the old stall but gave up aft...</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-05-01</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>Dinner</td>\n",
              "      <td>The Soy Chicken was nice. The rice was fragran...</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-05-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>444</th>\n",
              "      <td>Lunch</td>\n",
              "      <td>Lunched at Michelin's stars awarded Hawker Cha...</td>\n",
              "      <td>5</td>\n",
              "      <td>2017-04-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>471</th>\n",
              "      <td>Very good</td>\n",
              "      <td>A very good chicken and rice restaurant. Long ...</td>\n",
              "      <td>4</td>\n",
              "      <td>2017-01-01</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "              header  ... is_mobile_user\n",
              "60   Nothing special  ...          False\n",
              "73   Nothing special  ...           True\n",
              "75    Michelin star?  ...          False\n",
              "99    Michelin star?  ...           True\n",
              "113    Disappointing  ...           True\n",
              "117        Very good  ...           True\n",
              "160    Disappointing  ...           True\n",
              "171        Delicious  ...           True\n",
              "239            Lunch  ...           True\n",
              "339    Disappointing  ...           True\n",
              "383           Dinner  ...           True\n",
              "385    Disappointing  ...           True\n",
              "425        Delicious  ...          False\n",
              "426           Dinner  ...           True\n",
              "444            Lunch  ...           True\n",
              "471        Very good  ...           True\n",
              "\n",
              "[16 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qPoZPnVu4nm",
        "colab_type": "text"
      },
      "source": [
        "Perhaps not, from the instances where the headers were the identical we had differing review numbers. However, the range between the reviews are close - indicating that commenters who made similar comments had the similar general viewpoints in their ratings. This does suggest that more information is needed from the body to supplement the analysis of review patterns. The last thing to note is that frequency of synonymous headers are rare and since the reviews are similar, it might be forgiveable to get away with just analysing the headers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufacJgF7u4nn",
        "colab_type": "text"
      },
      "source": [
        "Let's now consider the effect of the comments on the ratings (the `review_num` column): "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpCrKO75u4no",
        "colab_type": "text"
      },
      "source": [
        "First, we need to hold out a test set in order to be able to gauge the generalisation ability of our model:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1hldIqMPu4np",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "3161beaa-81de-435a-d288-a24af36309d2"
      },
      "source": [
        "#Isolate Target From Predictors\n",
        "\n",
        "pred = data.drop([\"review_num\",\"review_date\",\"is_mobile_user\"],axis=1)\n",
        "target = data.review_num\n",
        "\n",
        "# create training and testing vars using a random seed to reproduce experiment results\n",
        "X_train, X_test, y_train, y_test = train_test_split(pred, target, test_size=0.2, random_state=5)\n",
        "\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(382, 2) (382,)\n",
            "(96, 2) (96,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rBqQ6OUou4nu",
        "colab_type": "text"
      },
      "source": [
        "## Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sLjaWSmbu4nv",
        "colab_type": "text"
      },
      "source": [
        "### Stopword and Punctuation Removal"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "741psplVu4nw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 538
        },
        "outputId": "f35a6604-b2fb-4109-bf85-a8e8f038f01e"
      },
      "source": [
        "def remove_stop_words_punc(data_string):\n",
        "    all_stopwords = stopwords.words(\"english\") # from nltk\n",
        "    puncs = list(string.punctuation)\n",
        "    puncs += \"â€™\"\n",
        "\n",
        "    final = []\n",
        "    text = word_tokenize(data_string.lower())\n",
        "    for token in text:\n",
        "        if token not in all_stopwords and token not in puncs:\n",
        "            final.append(token)\n",
        "\n",
        "    return ' '.join(final)\n",
        "\n",
        "print(\"Body:\",X_train[\"body\"].apply(remove_stop_words_punc),\"\\n\",sep=\"\\n\")\n",
        "print(\"Header:\",X_train[\"header\"].apply(remove_stop_words_punc),\"\\n\",sep=\"\\n\")"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Body:\n",
            "312    pretty good pork belly chicken good michelin s...\n",
            "296    else say ... 5 michelin star meal little hard ...\n",
            "330    judge local asian food seriously understand pl...\n",
            "176    singapore full hawker food part experience go ...\n",
            "305    went toa payoh outlet ordered mixed platter ro...\n",
            "                             ...                        \n",
            "400    visited tai seng outlet lunch still queue thou...\n",
            "118    sign front says michelin star queues inside at...\n",
            "189    ate 2 nights singapore recently went almost cl...\n",
            "206    good taste tried noodle rice vendor read vendo...\n",
            "355    plain good something would expect michelin sta...\n",
            "Name: body, Length: 382, dtype: object\n",
            "\n",
            "\n",
            "Header:\n",
            "312                       great pork belly so-so chicken\n",
            "296                                      5 michelin star\n",
            "330                                 michel-in michel-out\n",
            "176                                                 hype\n",
            "305                                     good value taste\n",
            "                             ...                        \n",
            "400                          lunch visit tai seng branch\n",
            "118    either michelin greay day mr chan indifferent one\n",
            "189                                   awesome cheap eats\n",
            "206                                                great\n",
            "355             cheapest michelin start restaurant world\n",
            "Name: header, Length: 382, dtype: object\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sbpT6L6Ku4n1",
        "colab_type": "text"
      },
      "source": [
        "### Stemming  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QC6benSMu4n1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "910803fc-e061-423f-dd4d-75001efe4464"
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "\n",
        "def stem_word(data_text):\n",
        "    return \" \".join([porter.stem(word) for word in word_tokenize(data_text)])\n",
        "\n",
        "stem_word(X_train.loc[35,\"header\"])"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'tasti street food , friendli staff'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEXuQPPku4n_",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatizing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j5AQjeT7u4oA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "outputId": "3cdf7996-e7f5-48ae-ecd0-b4326b1d4c83"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "def lemmatize_word(data_text):\n",
        "    return \" \".join([lemma.lemmatize(word) for word in word_tokenize(data_text)])\n",
        "\n",
        "lemmatize_word(X_train.loc[35,\"header\"])"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tasty street food , friendly staff'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a1yIWd0zu4oG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "8a13d362-3fb8-4902-90a8-4fab930bf58f"
      },
      "source": [
        "data.loc[35,:]"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "header                            Tasty street food, friendly staff\n",
              "body              I donâ€˜t know why snobbish people are going on ...\n",
              "review_num                                                        5\n",
              "review_date                                              2019-11-01\n",
              "is_mobile_user                                                 True\n",
              "Name: 35, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m37t9rTqu4oL",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RJ1OKvrwu4oL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def TextPreprocessorFn(text,strategy=\"lemma\"):\n",
        "    lemma = WordNetLemmatizer() \n",
        "    stemmer = PorterStemmer()\n",
        "    all_stopwords = stopwords.words(\"english\")\n",
        "    puncs = list(string.punctuation)\n",
        "    puncs += [\"â€œ\",\"â€™\",\"â€\",\"-\"]\n",
        "    doc = word_tokenize(text.lower())\n",
        "    removed = [t for t in doc if (t not in puncs) and (t not in all_stopwords)]\n",
        "    if strategy == \"lemma\":\n",
        "        return \" \".join([lemma.lemmatize(token) for token in removed])\n",
        "    else:\n",
        "        return \" \".join([stemmer.stem(token) for token in removed])"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8e-UU6Reu4oP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "e6f18c64-a755-4c8b-f439-cc326804d2d2"
      },
      "source": [
        "X_train[\"header\"].apply(TextPreprocessorFn,strategy=\"lemma\").values"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['great pork belly so-so chicken', '5 michelin star',\n",
              "       'michel-in michel-out', 'hype', 'good value taste', 'tragic',\n",
              "       'michelin star', 'original pretty amazing',\n",
              "       'singapore one find chicken rice 2 plate',\n",
              "       'best chicken rice ever', \"wo n't find michellin star street food\",\n",
              "       'fast food michelin star', 'eaten better food', 'much',\n",
              "       'hawker chan famous chicken', 'cool try worth hype',\n",
              "       'good experience', 'michelin star',\n",
              "       'super sedap soya chicken noodle',\n",
              "       \"terrible service horrible food michelin worthy n't bother\",\n",
              "       'family lunch', 'cheap tasty -- life reputation', 'fantastic',\n",
              "       'overhyped', 'rude bad service', 'worth',\n",
              "       'michelin star chicken rice noodle', 'good must visit', 'alright',\n",
              "       'tasty chicken', 'disappointing given hype', 'ok nothing special',\n",
              "       'superb comfort food covid-19', 'nothing special',\n",
              "       \"it'e everything need\", 'could use quality control',\n",
              "       'cheapest michelin star restaurant',\n",
              "       \"yes n't worry queue get line\", 'sure fuss', 'awesome meal',\n",
              "       'cheap michelin star restaurant', 'diasppointing overrated',\n",
              "       'hawker stall preferable taste', 'tasteless chicken',\n",
              "       'signature dish cold measly portion fragmented bone',\n",
              "       'cheapest good michelin meal sing',\n",
              "       'feel hardworking chef trying achieve accolade',\n",
              "       'cheapest 1 star michelin -- quite good',\n",
              "       'michelin food 5 singapore dollar/plate', 'worth seeking',\n",
              "       'good noodle', 'worth hype', 'stick chicken', 'cheap n good eats',\n",
              "       'first definitely worth', 'michelin star meal', 'excellent food',\n",
              "       'worth wait', 'michelin well deserved',\n",
              "       'cheapest michelin star experience', 'food fine nothing special',\n",
              "       'great food luke warm', 'totally overrated',\n",
              "       '1-michelin star meal incredible story', 'cold average',\n",
              "       'michelin meh', 'forget hype', 'take care dangerous health',\n",
              "       'excellent food value', 'lovely food price pat',\n",
              "       'unfortunately live hype', 'everything expected', 'disappointing',\n",
              "       'delicious food served', 'tasteless price winning menu',\n",
              "       'yummy chicken rice', 'forget char siu know',\n",
              "       'great food fast aervice', 'super food lived hype',\n",
              "       'take michelin star back', 'dinner hawker chan tai seng branch',\n",
              "       'soya chicken rice ... yum', 'overhyped michelin star craze',\n",
              "       'post mediterranean baltic cruise',\n",
              "       'michelin way good hawker quality', 'short changed',\n",
              "       'michelin star street food really good', 'cold food', 'lunch',\n",
              "       '4-stars go soya sauce chicken', 'michelin starred restaurant',\n",
              "       'quality assured chef chan', 'good ...',\n",
              "       'best soy sauce chicken -- deserving michelin star',\n",
              "       'great food comfy environment', 'disappointed',\n",
              "       'best chicken ever affordable price',\n",
              "       'michelin star food street stall price',\n",
              "       'delicious soy sauce chicken noodle',\n",
              "       'great soya chicken michelin star bargain',\n",
              "       'good cheap michelin experience', 'hawker chan funan',\n",
              "       'hong kong soy sauce chicken',\n",
              "       \"michelin star mcdonald 's better ...\",\n",
              "       'michelin star got excellent food ...', 'delicious',\n",
              "       'choking hazard zero star could', \"tried still n't find special\",\n",
              "       \"rip n't make mistake sucked\", 'disappointing meal',\n",
              "       'didnt wow 2nd n 3rd visit', 'good price', 'must visit singapore',\n",
              "       'place soy sauce chicken', 'chicken melt mouth',\n",
              "       'michelin little joke', 'succulent chicken', 'michelin star',\n",
              "       'bony busy', 'tasty chicken michelin star', 'branch good',\n",
              "       \"'s michelin star\", 'visited funan branch',\n",
              "       \"'s cheap disappointed lot better place eat price\",\n",
              "       'sampler meat platter worth wait line', 'good quality clean fast',\n",
              "       'sorry michelin', 'awesome pork', 'sure 1 michelin star',\n",
              "       'must try mediocre', 'michelin laugh',\n",
              "       'good value quite nice maybe overrated', 'michelin get right',\n",
              "       'authentic chinese fastfood michelin star', 'completely worth',\n",
              "       'seriously', 'little disappointed', 'pleased came experience..',\n",
              "       'subpar food', 'avoid farce', 'worth waiting', 'good food',\n",
              "       'chan special soya chicken good value', 'bad bad bad bad',\n",
              "       'food speaks volume service', 'hawker chan good',\n",
              "       'okay good expected', 'dinner',\n",
              "       \"still michelin stared restaurant 'm charlie chan earl derr biggers\",\n",
              "       'glad tried finish', 'bit disappointment honest',\n",
              "       'missed michelin mark', 'hawker food', 'incredible value',\n",
              "       'food consistent ...', 'believe hype', 'hype',\n",
              "       'really unique michelin 1* restaurant', 'great food super cheap',\n",
              "       \"order noodle 're hurry\", 'michelin budget',\n",
              "       'good enough price new hawker chan outlet funan mall',\n",
              "       'umm seriously michelin', 'bit strange michelin star',\n",
              "       'best soya sauce chicken charsiew michelin star',\n",
              "       'harrassed gangster china waiter toa payoh branch',\n",
              "       'hype substance', 'michelin preferred',\n",
              "       \"could n't finish it-undercooked chicken\",\n",
              "       'restaurant get awarded michelin star',\n",
              "       'dining cheapest michelin star restaurant',\n",
              "       'believe everything read', 'simple tasty', 'michelin star ðŸ‘',\n",
              "       'michelin starred', 'super disappointing',\n",
              "       'toa payoh central outlet', 'cheapest michelin star nice meal',\n",
              "       'excellentl good hawker food', 'michelin star experience',\n",
              "       'cheap meal good quality', 'starred', 'worth visit',\n",
              "       'delicious long long queue',\n",
              "       'cold food poor service fast food style restaurant',\n",
              "       'different kind chicken rice', 'average', 'major let',\n",
              "       'overated hawker chan', 'famous soy sauce chicken',\n",
              "       'hawker michelin food', 'de-licious', 'ridiculous waiting time',\n",
              "       'zero star food worse service', 'bad choise', 'definielty go',\n",
              "       'best', 'overhyped average hawker fare', 'served cold',\n",
              "       'delicious expected', \"'s good 's good\", 'okay', 'good value',\n",
              "       'hype exceptional taste nothing shout', 'great chicken indeed',\n",
              "       'terrible soya chicken', 'cheap eats meat', 'rated',\n",
              "       'outlet kuala lumpur', 'great chicken noodle',\n",
              "       'decent food would recommend', 'little over-hyped good food',\n",
              "       'disappointing', 'cheapest michelin star eatery world',\n",
              "       'decent fast food', 'tad disappointing',\n",
              "       'believe hype go anywhere',\n",
              "       'fresh hawker food sauce given deserved 1 michelin star rating',\n",
              "       'really enjoyable', 'delicious', 'michelin star surely',\n",
              "       'high quality soya chicken', 'michelin star hawker',\n",
              "       'street food good quality', 'experience',\n",
              "       'one star michelin star sure', 'disgusting',\n",
              "       'ordinary michelin star', 'terrible', 'good food overated',\n",
              "       'actually bad ...', 'ok forget michelin star bit', 'worth trying',\n",
              "       'good', 'char siew way better popular soy sauce chicken',\n",
              "       'best hawker stall', 'really', 'best chicken ever- michelin star',\n",
              "       'expectation', 'unfortunatelly ... poor',\n",
              "       'authentic singaporean food', 'much write', 'ok chicken',\n",
              "       'definitely skip toa payoh outlet', 'good chicken rice',\n",
              "       'tasty worth wait', 'good high expectation',\n",
              "       'best cheap meal city',\n",
              "       'manage expectation enjoy good food rather sterile atmosphere',\n",
              "       'chicken char siu', 'late lunch', 'bone red marrow',\n",
              "       'char siew delicious chicken average', 'okay expected',\n",
              "       'outlet toa payoh hub', 'singapore must-do', 'missing something',\n",
              "       'lunch', 'really good char siew soy chicken',\n",
              "       \"cheapest michelin star 'restaurant world\", '', 'disappointing',\n",
              "       'overpriced bad service', 'michelin bib â€˜ starred one',\n",
              "       'great food', 'tasty street food friendly staff', 'amazing',\n",
              "       'disappointing michelin experience toa payoh outlet',\n",
              "       'dissapoiting', 'simple yet superb', 'toa payoh outlet',\n",
              "       'cheap affordable michelin lunch', 'really good', 'overstated',\n",
              "       'michelin star', 'good value worth hype',\n",
              "       \"next michelin star force mcdonald 's\",\n",
              "       \"michelin star worthy n't know\", 'liao fan hawker chan',\n",
              "       'unbelievably cheap', 'quick lunch', \"best oily chicken 've ever\",\n",
              "       'delicious chicken rice', 'go experience food', 'could better',\n",
              "       'worm dish terrible hygiene', 'affordable michelin star',\n",
              "       'glorified hawker food', 'michelin star award',\n",
              "       'cheapest michelin-star dish', 'good chicken 1 michelin star',\n",
              "       'cold food new place', 'good', 'cold food ... bad service',\n",
              "       'hawker fare going upmarket', 'good cheap food',\n",
              "       'food standard standardized', 'nice good expected',\n",
              "       'fun novelty great food', 'delicious food.. sooo good',\n",
              "       '45minute wait worth every minute',\n",
              "       \"cheapest michelin star food 'll ever find\",\n",
              "       'tasty springy noodle', 'michelin judge panel thinking',\n",
              "       'pork excellent chicken ok', 'victim success',\n",
              "       'terrible food horrible service', 'experience missed',\n",
              "       'hawker chan smith street',\n",
              "       'temporary bout insanity befalls michelin reviewer', 'sure fuss',\n",
              "       'fun food', 'deserve michelin star 1 hype', 'amazing food',\n",
              "       'love soyasauce chicken noodle', \"'s lived\",\n",
              "       'badly burnt char siew sold', 'inconsistent',\n",
              "       'reputation alone justify accolade', 'try',\n",
              "       '3rd time chef chan shop', 'fresh cheap excellent china town food',\n",
              "       'worth price', 'tasty', 'go early beat queue',\n",
              "       \"best bbq pork pork belly 've\", 'award worthy soy sauce chicken',\n",
              "       'ok expectation', '5/5',\n",
              "       'well hawker chan made history ... wanted share',\n",
              "       'find anything special', 'tasty hawker food', 'over-hyped', 'good',\n",
              "       'tasty chicken best soya sauce noodle', 'average cheap',\n",
              "       'expectation high', \"'s michelin star\", 'michelin star',\n",
              "       'tasty fast', 'cheapest michelin star ever', 'chicken noodle make',\n",
              "       'authentic singapore soya chicken', 'try', 'chicken good',\n",
              "       'alright', 'disappointing', 'reasonable fair', 'great seems',\n",
              "       'incredible food', 'cheap good', 'crowded',\n",
              "       'overrated comfort food',\n",
              "       'get better deal original hawker stall chinatown complex',\n",
              "       'nice food michelin-starred', 'bibendum got one wrong methinks',\n",
              "       \"michelin star restaurant wo n't hurt wallet\", 'great value',\n",
              "       'sunday dinner toa payoh branch', 'worth hype',\n",
              "       'quick cheap reasonable quality meal', 'terrible service', 'must',\n",
              "       'authentic dish worth waiting', 'disappointing',\n",
              "       'good food fine dining', 'good visit', 'live hype',\n",
              "       'michelin star hawker stand',\n",
              "       'cheapest michelins starred restaurant evenr',\n",
              "       'rushed staff equal inconsistent product', 'little actual meat',\n",
              "       'good say', 'over-rated dish crowned michelin', 'prepared queue',\n",
              "       'singapore michelin-starred chicken rice', 'nothing special',\n",
              "       'lunch visit tai seng branch',\n",
              "       'either michelin greay day mr chan indifferent one',\n",
              "       'awesome cheap eats', 'great',\n",
              "       'cheapest michelin start restaurant world'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FB2XJFL4u4oT",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together into a Transformer (Optional)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOBwfTPzu4oT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.base import TransformerMixin, BaseEstimator\n",
        "\n",
        "class TextPreprocessor(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self,strategy=\"lemma\"):\n",
        "        \"\"\"\n",
        "        Text preprocessing transformer includes steps:\n",
        "            1. Text normalization\n",
        "            2. Punctuation removal\n",
        "            3. Stop words removal\n",
        "            4. Lemmatization\n",
        "        \"\"\"\n",
        "        self.strategy = strategy\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self._lemma = WordNetLemmatizer() \n",
        "        self._stemmer = PorterStemmer()\n",
        "        self._all_stopwords = stopwords.words(\"english\")\n",
        "        self._puncs = list(string.punctuation)\n",
        "        self._puncs += [\"â€œ\",\"â€™\",\"â€\",\"-\"]\n",
        "        self._vectorized_process = np.vectorize(self._preprocess_text)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, *_):\n",
        "        X_copy = X.copy().values\n",
        "        \n",
        "        return self._vectorized_process(X_copy)\n",
        "    \n",
        "    \n",
        "    def _preprocess_text(self, text):\n",
        "        doc = word_tokenize(text.lower())\n",
        "        removed_punct = self._remove_punct(doc)\n",
        "        removed_stop_words = self._remove_stop_words(removed_punct)\n",
        "        if self.strategy == \"lemma\":\n",
        "            return self._lemmatize(removed_stop_words)\n",
        "        else:\n",
        "            return self._stem(removed_stop_words)\n",
        "\n",
        "    def _remove_punct(self, doc):\n",
        "        return [t for t in doc if t not in self._puncs]\n",
        "\n",
        "    def _remove_stop_words(self, doc):\n",
        "        return [t for t in doc if t not in self._all_stopwords]\n",
        "\n",
        "    def _lemmatize(self, doc):\n",
        "        return ' '.join([self._lemma.lemmatize(t) for t in doc])\n",
        "    \n",
        "    def _stem(self,doc):\n",
        "        return ' '.join([self._stemmer.stem(t) for t in doc])\n",
        "    "
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQ3mE61ayq_k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0f47592b-f129-4811-ae41-65564e460c34"
      },
      "source": [
        "text_preprocessor = TextPreprocessor()\n",
        "text_preprocessor.fit_transform(X_train[\"header\"])"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['great pork belly so-so chicken', '5 michelin star',\n",
              "       'michel-in michel-out', 'hype', 'good value taste', 'tragic',\n",
              "       'michelin star', 'original pretty amazing',\n",
              "       'singapore one find chicken rice 2 plate',\n",
              "       'best chicken rice ever', \"wo n't find michellin star street food\",\n",
              "       'fast food michelin star', 'eaten better food', 'much',\n",
              "       'hawker chan famous chicken', 'cool try worth hype',\n",
              "       'good experience', 'michelin star',\n",
              "       'super sedap soya chicken noodle',\n",
              "       \"terrible service horrible food michelin worthy n't bother\",\n",
              "       'family lunch', 'cheap tasty -- life reputation', 'fantastic',\n",
              "       'overhyped', 'rude bad service', 'worth',\n",
              "       'michelin star chicken rice noodle', 'good must visit', 'alright',\n",
              "       'tasty chicken', 'disappointing given hype', 'ok nothing special',\n",
              "       'superb comfort food covid-19', 'nothing special',\n",
              "       \"it'e everything need\", 'could use quality control',\n",
              "       'cheapest michelin star restaurant',\n",
              "       \"yes n't worry queue get line\", 'sure fuss', 'awesome meal',\n",
              "       'cheap michelin star restaurant', 'diasppointing overrated',\n",
              "       'hawker stall preferable taste', 'tasteless chicken',\n",
              "       'signature dish cold measly portion fragmented bone',\n",
              "       'cheapest good michelin meal sing',\n",
              "       'feel hardworking chef trying achieve accolade',\n",
              "       'cheapest 1 star michelin -- quite good',\n",
              "       'michelin food 5 singapore dollar/plate', 'worth seeking',\n",
              "       'good noodle', 'worth hype', 'stick chicken', 'cheap n good eats',\n",
              "       'first definitely worth', 'michelin star meal', 'excellent food',\n",
              "       'worth wait', 'michelin well deserved',\n",
              "       'cheapest michelin star experience', 'food fine nothing special',\n",
              "       'great food luke warm', 'totally overrated',\n",
              "       '1-michelin star meal incredible story', 'cold average',\n",
              "       'michelin meh', 'forget hype', 'take care dangerous health',\n",
              "       'excellent food value', 'lovely food price pat',\n",
              "       'unfortunately live hype', 'everything expected', 'disappointing',\n",
              "       'delicious food served', 'tasteless price winning menu',\n",
              "       'yummy chicken rice', 'forget char siu know',\n",
              "       'great food fast aervice', 'super food lived hype',\n",
              "       'take michelin star back', 'dinner hawker chan tai seng branch',\n",
              "       'soya chicken rice ... yum', 'overhyped michelin star craze',\n",
              "       'post mediterranean baltic cruise',\n",
              "       'michelin way good hawker quality', 'short changed',\n",
              "       'michelin star street food really good', 'cold food', 'lunch',\n",
              "       '4-stars go soya sauce chicken', 'michelin starred restaurant',\n",
              "       'quality assured chef chan', 'good ...',\n",
              "       'best soy sauce chicken -- deserving michelin star',\n",
              "       'great food comfy environment', 'disappointed',\n",
              "       'best chicken ever affordable price',\n",
              "       'michelin star food street stall price',\n",
              "       'delicious soy sauce chicken noodle',\n",
              "       'great soya chicken michelin star bargain',\n",
              "       'good cheap michelin experience', 'hawker chan funan',\n",
              "       'hong kong soy sauce chicken',\n",
              "       \"michelin star mcdonald 's better ...\",\n",
              "       'michelin star got excellent food ...', 'delicious',\n",
              "       'choking hazard zero star could', \"tried still n't find special\",\n",
              "       \"rip n't make mistake sucked\", 'disappointing meal',\n",
              "       'didnt wow 2nd n 3rd visit', 'good price', 'must visit singapore',\n",
              "       'place soy sauce chicken', 'chicken melt mouth',\n",
              "       'michelin little joke', 'succulent chicken', 'michelin star',\n",
              "       'bony busy', 'tasty chicken michelin star', 'branch good',\n",
              "       \"'s michelin star\", 'visited funan branch',\n",
              "       \"'s cheap disappointed lot better place eat price\",\n",
              "       'sampler meat platter worth wait line', 'good quality clean fast',\n",
              "       'sorry michelin', 'awesome pork', 'sure 1 michelin star',\n",
              "       'must try mediocre', 'michelin laugh',\n",
              "       'good value quite nice maybe overrated', 'michelin get right',\n",
              "       'authentic chinese fastfood michelin star', 'completely worth',\n",
              "       'seriously', 'little disappointed', 'pleased came experience..',\n",
              "       'subpar food', 'avoid farce', 'worth waiting', 'good food',\n",
              "       'chan special soya chicken good value', 'bad bad bad bad',\n",
              "       'food speaks volume service', 'hawker chan good',\n",
              "       'okay good expected', 'dinner',\n",
              "       \"still michelin stared restaurant 'm charlie chan earl derr biggers\",\n",
              "       'glad tried finish', 'bit disappointment honest',\n",
              "       'missed michelin mark', 'hawker food', 'incredible value',\n",
              "       'food consistent ...', 'believe hype', 'hype',\n",
              "       'really unique michelin 1* restaurant', 'great food super cheap',\n",
              "       \"order noodle 're hurry\", 'michelin budget',\n",
              "       'good enough price new hawker chan outlet funan mall',\n",
              "       'umm seriously michelin', 'bit strange michelin star',\n",
              "       'best soya sauce chicken charsiew michelin star',\n",
              "       'harrassed gangster china waiter toa payoh branch',\n",
              "       'hype substance', 'michelin preferred',\n",
              "       \"could n't finish it-undercooked chicken\",\n",
              "       'restaurant get awarded michelin star',\n",
              "       'dining cheapest michelin star restaurant',\n",
              "       'believe everything read', 'simple tasty', 'michelin star ðŸ‘',\n",
              "       'michelin starred', 'super disappointing',\n",
              "       'toa payoh central outlet', 'cheapest michelin star nice meal',\n",
              "       'excellentl good hawker food', 'michelin star experience',\n",
              "       'cheap meal good quality', 'starred', 'worth visit',\n",
              "       'delicious long long queue',\n",
              "       'cold food poor service fast food style restaurant',\n",
              "       'different kind chicken rice', 'average', 'major let',\n",
              "       'overated hawker chan', 'famous soy sauce chicken',\n",
              "       'hawker michelin food', 'de-licious', 'ridiculous waiting time',\n",
              "       'zero star food worse service', 'bad choise', 'definielty go',\n",
              "       'best', 'overhyped average hawker fare', 'served cold',\n",
              "       'delicious expected', \"'s good 's good\", 'okay', 'good value',\n",
              "       'hype exceptional taste nothing shout', 'great chicken indeed',\n",
              "       'terrible soya chicken', 'cheap eats meat', 'rated',\n",
              "       'outlet kuala lumpur', 'great chicken noodle',\n",
              "       'decent food would recommend', 'little over-hyped good food',\n",
              "       'disappointing', 'cheapest michelin star eatery world',\n",
              "       'decent fast food', 'tad disappointing',\n",
              "       'believe hype go anywhere',\n",
              "       'fresh hawker food sauce given deserved 1 michelin star rating',\n",
              "       'really enjoyable', 'delicious', 'michelin star surely',\n",
              "       'high quality soya chicken', 'michelin star hawker',\n",
              "       'street food good quality', 'experience',\n",
              "       'one star michelin star sure', 'disgusting',\n",
              "       'ordinary michelin star', 'terrible', 'good food overated',\n",
              "       'actually bad ...', 'ok forget michelin star bit', 'worth trying',\n",
              "       'good', 'char siew way better popular soy sauce chicken',\n",
              "       'best hawker stall', 'really', 'best chicken ever- michelin star',\n",
              "       'expectation', 'unfortunatelly ... poor',\n",
              "       'authentic singaporean food', 'much write', 'ok chicken',\n",
              "       'definitely skip toa payoh outlet', 'good chicken rice',\n",
              "       'tasty worth wait', 'good high expectation',\n",
              "       'best cheap meal city',\n",
              "       'manage expectation enjoy good food rather sterile atmosphere',\n",
              "       'chicken char siu', 'late lunch', 'bone red marrow',\n",
              "       'char siew delicious chicken average', 'okay expected',\n",
              "       'outlet toa payoh hub', 'singapore must-do', 'missing something',\n",
              "       'lunch', 'really good char siew soy chicken',\n",
              "       \"cheapest michelin star 'restaurant world\", '', 'disappointing',\n",
              "       'overpriced bad service', 'michelin bib â€˜ starred one',\n",
              "       'great food', 'tasty street food friendly staff', 'amazing',\n",
              "       'disappointing michelin experience toa payoh outlet',\n",
              "       'dissapoiting', 'simple yet superb', 'toa payoh outlet',\n",
              "       'cheap affordable michelin lunch', 'really good', 'overstated',\n",
              "       'michelin star', 'good value worth hype',\n",
              "       \"next michelin star force mcdonald 's\",\n",
              "       \"michelin star worthy n't know\", 'liao fan hawker chan',\n",
              "       'unbelievably cheap', 'quick lunch', \"best oily chicken 've ever\",\n",
              "       'delicious chicken rice', 'go experience food', 'could better',\n",
              "       'worm dish terrible hygiene', 'affordable michelin star',\n",
              "       'glorified hawker food', 'michelin star award',\n",
              "       'cheapest michelin-star dish', 'good chicken 1 michelin star',\n",
              "       'cold food new place', 'good', 'cold food ... bad service',\n",
              "       'hawker fare going upmarket', 'good cheap food',\n",
              "       'food standard standardized', 'nice good expected',\n",
              "       'fun novelty great food', 'delicious food.. sooo good',\n",
              "       '45minute wait worth every minute',\n",
              "       \"cheapest michelin star food 'll ever find\",\n",
              "       'tasty springy noodle', 'michelin judge panel thinking',\n",
              "       'pork excellent chicken ok', 'victim success',\n",
              "       'terrible food horrible service', 'experience missed',\n",
              "       'hawker chan smith street',\n",
              "       'temporary bout insanity befalls michelin reviewer', 'sure fuss',\n",
              "       'fun food', 'deserve michelin star 1 hype', 'amazing food',\n",
              "       'love soyasauce chicken noodle', \"'s lived\",\n",
              "       'badly burnt char siew sold', 'inconsistent',\n",
              "       'reputation alone justify accolade', 'try',\n",
              "       '3rd time chef chan shop', 'fresh cheap excellent china town food',\n",
              "       'worth price', 'tasty', 'go early beat queue',\n",
              "       \"best bbq pork pork belly 've\", 'award worthy soy sauce chicken',\n",
              "       'ok expectation', '5/5',\n",
              "       'well hawker chan made history ... wanted share',\n",
              "       'find anything special', 'tasty hawker food', 'over-hyped', 'good',\n",
              "       'tasty chicken best soya sauce noodle', 'average cheap',\n",
              "       'expectation high', \"'s michelin star\", 'michelin star',\n",
              "       'tasty fast', 'cheapest michelin star ever', 'chicken noodle make',\n",
              "       'authentic singapore soya chicken', 'try', 'chicken good',\n",
              "       'alright', 'disappointing', 'reasonable fair', 'great seems',\n",
              "       'incredible food', 'cheap good', 'crowded',\n",
              "       'overrated comfort food',\n",
              "       'get better deal original hawker stall chinatown complex',\n",
              "       'nice food michelin-starred', 'bibendum got one wrong methinks',\n",
              "       \"michelin star restaurant wo n't hurt wallet\", 'great value',\n",
              "       'sunday dinner toa payoh branch', 'worth hype',\n",
              "       'quick cheap reasonable quality meal', 'terrible service', 'must',\n",
              "       'authentic dish worth waiting', 'disappointing',\n",
              "       'good food fine dining', 'good visit', 'live hype',\n",
              "       'michelin star hawker stand',\n",
              "       'cheapest michelins starred restaurant evenr',\n",
              "       'rushed staff equal inconsistent product', 'little actual meat',\n",
              "       'good say', 'over-rated dish crowned michelin', 'prepared queue',\n",
              "       'singapore michelin-starred chicken rice', 'nothing special',\n",
              "       'lunch visit tai seng branch',\n",
              "       'either michelin greay day mr chan indifferent one',\n",
              "       'awesome cheap eats', 'great',\n",
              "       'cheapest michelin start restaurant world'], dtype='<U66')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2nrlv7PKu4oY",
        "colab_type": "text"
      },
      "source": [
        "### Creating the Training and Testing Tf-Idf Matrices"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yCWTzU6u4oZ",
        "colab_type": "text"
      },
      "source": [
        "#### Using the `TfidfVectorizer` itself:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QJeoE6Ru4oa",
        "colab_type": "text"
      },
      "source": [
        "First, initialize the vectorizer (be sure to add the Text Preprocessing Function as a parameter). Then fit and transform the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xgdwQXvu4od",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer_head = TfidfVectorizer(preprocessor=TextPreprocessorFn,min_df=5, max_df=0.7)\n",
        "vectorizer_body = TfidfVectorizer(preprocessor=TextPreprocessorFn,min_df=5, max_df=0.7)\n",
        "tfidf_matrix_header_train = vectorizer_head.fit_transform(X_train[\"header\"])\n",
        "tfidf_matrix_body_train = vectorizer_body.fit_transform(X_train[\"body\"])\n",
        "\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Md5ORxOku4oj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "8e41e464-5a98-426c-99db-f078793470ea"
      },
      "source": [
        "print(len(vectorizer_head.get_feature_names()))\n",
        "print(len(vectorizer_body.get_feature_names()))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57\n",
            "326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0INLOVOu4om",
        "colab_type": "text"
      },
      "source": [
        "Combine the two matrices together:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fn2Ztrx9u4om",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4bf70926-9d42-4c76-b09b-64f58f4e1af8"
      },
      "source": [
        "import scipy\n",
        "\n",
        "X_train_vec = scipy.sparse.hstack([tfidf_matrix_header_train,tfidf_matrix_body_train])\n",
        "\n",
        "print(X_train_vec.shape)\n"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(382, 383)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPPvd4QRu4pC",
        "colab_type": "text"
      },
      "source": [
        "Now using the same vectorizer, transform the test data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zFs_-kF2u4pD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "832c38d0-a367-4d10-faae-17ae2f041fbd"
      },
      "source": [
        "tfidf_matrix_header_test = vectorizer_head.transform(X_test[\"header\"])\n",
        "tfidf_matrix_body_test = vectorizer_body.transform(X_test[\"body\"])\n",
        "\n",
        "X_test_vec = scipy.sparse.hstack([tfidf_matrix_header_test,tfidf_matrix_body_test])\n",
        "\n",
        "print(X_test_vec.shape)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(96, 383)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Ca2qbV8u4pI",
        "colab_type": "text"
      },
      "source": [
        "#### Using the Transformer Method (Only applicable if you have created a transformer):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8D56LFdIu4pJ",
        "colab_type": "text"
      },
      "source": [
        "First, preprocess the data using the transformer. Then use the Tf-Idf vectorizer to fit and transform the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qXRrVO__u4pJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "processor = TextPreprocessor()\n",
        "processed_X_train = X_train.copy()\n",
        "processed_X_train.loc[:,\"header\"] = processor.fit_transform(X_train[\"header\"])\n",
        "processed_X_train.loc[:,\"body\"] = processor.fit_transform(X_train[\"body\"])\n",
        "\n",
        "processed_X_test = X_test.copy()\n",
        "processed_X_test.loc[:,\"header\"] = processor.fit_transform(X_test[\"header\"])\n",
        "processed_X_test.loc[:,\"body\"] = processor.fit_transform(X_test[\"body\"])\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "vectorizer_head = TfidfVectorizer(min_df=5, max_df=0.7)\n",
        "vectorizer_body = TfidfVectorizer(min_df=5, max_df=0.7)\n",
        "tfidf_matrix_header_train = vectorizer_head.fit_transform(processed_X_train[\"header\"])\n",
        "tfidf_matrix_body_train = vectorizer_body.fit_transform(processed_X_train[\"body\"])\n"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RCJt_sD0u4pN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "6577978a-b2a7-4c99-bcbd-d860bf6aac2d"
      },
      "source": [
        "print(len(vectorizer_head.get_feature_names()))\n",
        "print(len(vectorizer_body.get_feature_names()))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "57\n",
            "326\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "246sYQ_Cu4pQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6ccd7f00-b1fe-4869-c8ff-2e866e1654f8"
      },
      "source": [
        "import scipy\n",
        "\n",
        "X_train_vec = scipy.sparse.hstack([tfidf_matrix_header_train,tfidf_matrix_body_train])\n",
        "\n",
        "print(X_train_vec.shape)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(382, 383)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YJYahbxvu4pT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf_matrix_header_test = vectorizer_head.transform(processed_X_test[\"header\"])\n",
        "tfidf_matrix_body_test = vectorizer_body.transform(processed_X_test[\"body\"])\n",
        "\n",
        "X_test_vec = scipy.sparse.hstack([tfidf_matrix_header_test,tfidf_matrix_body_test])"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mpStLzUVu4pW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 903
        },
        "outputId": "8e5446f9-bd93-4a3a-acf9-ae30ddc85feb"
      },
      "source": [
        "print(tfidf_matrix_body_train)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  (0, 157)\t0.29373391789748027\n",
            "  (0, 307)\t0.30456928190755983\n",
            "  (0, 137)\t0.4710190245132066\n",
            "  (0, 314)\t0.2958035350168892\n",
            "  (0, 271)\t0.18056820434260834\n",
            "  (0, 174)\t0.16268481506488705\n",
            "  (0, 48)\t0.14985683984885734\n",
            "  (0, 212)\t0.27863544653978\n",
            "  (0, 124)\t0.439352129007043\n",
            "  (0, 215)\t0.3992530299946493\n",
            "  (1, 275)\t0.21069258889269624\n",
            "  (1, 235)\t0.2808655289565301\n",
            "  (1, 231)\t0.15252865358058867\n",
            "  (1, 155)\t0.2679925437198441\n",
            "  (1, 140)\t0.27407071337612515\n",
            "  (1, 156)\t0.28856885551813505\n",
            "  (1, 269)\t0.2679925437198441\n",
            "  (1, 132)\t0.13345289981625905\n",
            "  (1, 197)\t0.2251907310347061\n",
            "  (1, 160)\t0.28856885551813505\n",
            "  (1, 108)\t0.22800539925769311\n",
            "  (1, 131)\t0.2624941704516007\n",
            "  (1, 153)\t0.24460156369856614\n",
            "  (1, 171)\t0.17070544443054525\n",
            "  (1, 240)\t0.21513241402100713\n",
            "  :\t:\n",
            "  (379, 195)\t0.19610887079602962\n",
            "  (379, 256)\t0.18970907543816953\n",
            "  (379, 314)\t0.19886165523662352\n",
            "  (379, 48)\t0.10074517608172555\n",
            "  (380, 272)\t0.4449398916867505\n",
            "  (380, 225)\t0.5056800605012912\n",
            "  (380, 282)\t0.3941780182047317\n",
            "  (380, 297)\t0.4058872656681607\n",
            "  (380, 234)\t0.2549521277509335\n",
            "  (380, 187)\t0.3065527604008468\n",
            "  (380, 124)\t0.25926859504324873\n",
            "  (381, 94)\t0.3407128751758943\n",
            "  (381, 254)\t0.37455736361935454\n",
            "  (381, 188)\t0.312655960349934\n",
            "  (381, 284)\t0.26558596314937594\n",
            "  (381, 182)\t0.2997878730412323\n",
            "  (381, 151)\t0.24404442988860442\n",
            "  (381, 78)\t0.24219545566898223\n",
            "  (381, 187)\t0.20654655096690896\n",
            "  (381, 322)\t0.2707521253061833\n",
            "  (381, 256)\t0.2243986852605251\n",
            "  (381, 208)\t0.3734529296271688\n",
            "  (381, 174)\t0.12936801330812878\n",
            "  (381, 48)\t0.11916712475069476\n",
            "  (381, 124)\t0.17468782212300485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCxBgUb2u4pa",
        "colab_type": "text"
      },
      "source": [
        "### Creating the Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v0r7mmvFu4pa",
        "colab_type": "text"
      },
      "source": [
        "Let's now create a Logistic Regression Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7tGr-4Uu4pc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "model.fit(X_train_vec, y_train);"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aSQPPz0Lu4pf",
        "colab_type": "text"
      },
      "source": [
        "Score it against the training data:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw2elZpAu4pg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b7a075ed-ba76-4e3c-c17e-3a450fa6ff2e"
      },
      "source": [
        "model.score(X_train_vec, y_train)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.806282722513089"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "puEf_jFpu4pj",
        "colab_type": "text"
      },
      "source": [
        "Now use it to predict and score it against the test data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fJCNvDNPu4pk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "70756b58-282a-47c8-d681-2867fcb51a8a"
      },
      "source": [
        "preds = model.predict(X_test_vec)\n",
        "model.score(X_test_vec, y_test)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4166666666666667"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xe2O9xrmu4pm",
        "colab_type": "text"
      },
      "source": [
        "What do the results above tell you about the model?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIrU8xJIu4pn",
        "colab_type": "text"
      },
      "source": [
        "- A Logistic Regression Model (with Regularization) does not really work well for the problem of accurately predicting user ratings.\n",
        "- Overfitting can be seen.\n",
        "- Rationale: linear models might not be geometrically complex enough to separate the classes based on textual information.\n",
        "- Vectorizing int Tf-idf is a simplistic approach, which fails to capture contexts of words. Perhaps more complex transformers such as word embeddings can capture the nuances of the dataset better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfYlgN06u4po",
        "colab_type": "text"
      },
      "source": [
        "A proxy can be as such, instead of predicting the actual rating we can say that a prediction does well if it comes within a difference of one star:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i7bSvTPGu4po",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "fb303180-f89a-4c1c-dfe0-75a207ccdc87"
      },
      "source": [
        "np.mean(np.abs(y_test.values - preds)<2)"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8020833333333334"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPNnrwV5u4pr",
        "colab_type": "text"
      },
      "source": [
        "The result above tells us that while the model might be terrible in predicting the actual ratings of users. It does capture the essence of what the user feels based on the comments."
      ]
    }
  ]
}